@article{1-0,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}

@article{1-1,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}


@ARTICLE{1-2,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}

@article{1-2b1,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}
@article{1-2b2,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{1-2b3,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{1-3,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue-date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {1137–1155},
numpages = {19}
}
@article{1-3b1,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@article{1-4,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{1-5,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{1-6,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@inproceedings{1-7,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}
@article{1-8,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{1-9,
  title={Mass: Masked sequence to sequence pre-training for language generation},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1905.02450},
  year={2019}
}

@article{1-10,
  title={Beyond english-centric multilingual machine translation},
  author={Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and others},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={4839--4886},
  year={2021},
  publisher={JMLRORG}
}



%********************************************************************************************************


@article{2-1, author = {LUO Tianhua \& DENG Shuwen}, title = {Morphological Meanings and Language Forms: A Review on Jiang (2021)}, journal = {Minority Languages of China}, volume = {No.257}, number = {122-128}, year = {2022}, issn = {0257-5779}, doi ={} }

@article{2-1-1, author = {Song Jinlan}, title = {Differentiation of Morphological Variants in Chinese Tibetan Language}, journal = {Minority Languages of China}, volume = {}, number = {29-33}, year = {2002}, issn = {0257-5779}, doi ={} }

@article{2-2, author = {Mao Lei}, title = {A Morphological Study of Hindi}, school = {Information Engineering University}, year = {2022} }

@article{2-3, author = {Renqing-zhuome}, title = {Research on the Structure of Tibetan Verb Phrases
Based on Language Information Processing}, journal = {Plateau Science Research}, volume = {2}, number = {60-69}, year = {2018}, issn = {2096-4617}, doi ={10.16249/j.cnki.2096-4617.2018.04.008} }




%********************************************************************************************************
@article{3-2,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{3-3,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@article{3-4,
  title={Unsupervised neural machine translation},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1710.11041},
  year={2017}
}
@article{3-5,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
  year={2015}
}
@inproceedings{3-6,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

%************************************************LLM修改添加**********************************************************************************

@inproceedings{Jacob,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhebin,
  author       = {Zhebin Zhang and
                  Sai Wu and
                  Dawei Jiang and
                  Gang Chen},
  title        = {{BERT-JAM:} Maximizing the utilization of {BERT} for neural machine
                  translation},
  journal      = {Neurocomputing},
  volume       = {460},
  pages        = {84--94},
  year         = {2021},
  url          = {https://doi.org/10.1016/j.neucom.2021.07.002},
  doi          = {10.1016/J.NEUCOM.2021.07.002},
  timestamp    = {Fri, 24 Sep 2021 09:47:29 +0200},
  biburl       = {https://dblp.org/rec/journals/ijon/ZhangWJC21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Haoran,
  author       = {Haoran Xu and
                  Benjamin Van Durme and
                  Kenton W. Murray},
  editor       = {Marie{-}Francine Moens and
                  Xuanjing Huang and
                  Lucia Specia and
                  Scott Wen{-}tau Yih},
  title        = {BERT, mBERT, or BiBERT? {A} Study on Contextualized Embeddings for
                  Neural Machine Translation},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican
                  Republic, 7-11 November, 2021},
  pages        = {6663--6675},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.emnlp-main.534},
  doi          = {10.18653/V1/2021.EMNLP-MAIN.534},
  timestamp    = {Fri, 16 Feb 2024 08:27:36 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/XuDM21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Yinhan,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Thu, 14 Dec 2023 18:03:41 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Yu,
  author       = {Yu Sun and
                  Shuohuan Wang and
                  Yu{-}Kun Li and
                  Shikun Feng and
                  Hao Tian and
                  Hua Wu and
                  Haifeng Wang},
  title        = {{ERNIE} 2.0: {A} Continual Pre-Training Framework for Language Understanding},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {8968--8975},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i05.6428},
  doi          = {10.1609/AAAI.V34I05.6428},
  timestamp    = {Mon, 04 Sep 2023 16:50:27 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/SunWLFTWW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Xueqing,
  author       = {Xueqing Wu and
                  Yingce Xia and
                  Jinhua Zhu and
                  Lijun Wu and
                  Shufang Xie and
                  Tao Qin},
  title        = {A study of {BERT} for context-aware neural machine translation},
  journal      = {Mach. Learn.},
  volume       = {111},
  number       = {3},
  pages        = {917--935},
  year         = {2022},
  url          = {https://doi.org/10.1007/s10994-021-06070-y},
  doi          = {10.1007/S10994-021-06070-Y},
  timestamp    = {Mon, 29 Jan 2024 17:56:07 +0100},
  biburl       = {https://dblp.org/rec/journals/ml/WuXZWXQ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Mike,
  author       = {Mike Lewis and
                  Yinhan Liu and
                  Naman Goyal and
                  Marjan Ghazvininejad and
                  Abdelrahman Mohamed and
                  Omer Levy and
                  Veselin Stoyanov and
                  Luke Zettlemoyer},
  editor       = {Dan Jurafsky and
                  Joyce Chai and
                  Natalie Schluter and
                  Joel R. Tetreault},
  title        = {{BART:} Denoising Sequence-to-Sequence Pre-training for Natural Language
                  Generation, Translation, and Comprehension},
  booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2020, Online, July 5-10, 2020},
  pages        = {7871--7880},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  url          = {https://doi.org/10.18653/v1/2020.acl-main.703},
  doi          = {10.18653/V1/2020.ACL-MAIN.703},
  timestamp    = {Wed, 16 Mar 2022 23:55:02 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/LewisLGGMLSZ20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Moreno,
  author       = {Moreno La Quatra and
                  Luca Cagliero},
  title        = {{BART-IT:} An Efficient Sequence-to-Sequence Model for Italian Text
                  Summarization},
  journal      = {Future Internet},
  volume       = {15},
  number       = {1},
  pages        = {15},
  year         = {2023},
  url          = {https://doi.org/10.3390/fi15010015},
  doi          = {10.3390/FI15010015},
  timestamp    = {Tue, 31 Jan 2023 20:44:17 +0100},
  biburl       = {https://dblp.org/rec/journals/fi/QuatraC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article {Laki,
      author = "László János Laki and Zijian Győző Yang",
      title = "Neural machine translation for Hungarian",
      journal = "Acta Linguistica Academica",
      year = "2022",
      publisher = "Akadémiai Kiadó",
      address = "Budapest, Hungary",
      volume = "69",
      number = "4",
      doi = "10.1556/2062.2022.00576",
      pages=      "501 - 520",
      url = "https://akjournals.com/view/journals/2062/69/4/article-p501.xml"
}
@inproceedings{Navarro,
    title = "Exploring Multilingual Pretrained Machine Translation Models for Interactive Translation",
    author = "Navarro, Angel  and
      Casacuberta, Francisco",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.12",
    pages = "132--142",
    abstract = "Pre-trained large language models (LLM) constitute very important tools in many artificial intelligence applications. In this work, we explore the use of these models in interactive machine translation environments. In particular, we have chosen mBART (multilingual Bidirectional and Auto-Regressive Transformer) as one of these LLMs. The system enables users to refine the translation output interactively by providing feedback. The system utilizes a two-step process, where the NMT (Neural Machine Translation) model generates a preliminary translation in the first step, and the user performs one correction in the second step{--}repeating the process until the sentence is correctly translated. We assessed the performance of both mBART and the fine-tuned version by comparing them to a state-of-the-art machine translation model on a benchmark dataset regarding user effort, WSR (Word Stroke Ratio), and MAR (Mouse Action Ratio). The experimental results indicate that all the models performed comparably, suggesting that mBART is a viable option for an interactive machine translation environment, as it eliminates the need to train a model from scratch for this particular task. The implications of this finding extend to the development of new machine translation models for interactive environments, as it indicates that novel pre-trained models exhibit state-of-the-art performance in this domain, highlighting the potential benefits of adapting these models to specific needs.",
}

@inproceedings{Shiun,
  author       = {En{-}Shiun Annie Lee and
                  Sarubi Thillainathan and
                  Shravan Nayak and
                  Surangika Ranathunga and
                  David Ifeoluwa Adelani and
                  Ruisi Su and
                  Arya McCarthy},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {Pre-Trained Multilingual Sequence-to-Sequence Models: {A} Hope for
                  Low-Resource Language Translation?},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL} 2022,
                  Dublin, Ireland, May 22-27, 2022},
  pages        = {58--67},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-acl.6},
  doi          = {10.18653/V1/2022.FINDINGS-ACL.6},
  timestamp    = {Mon, 01 Aug 2022 16:27:49 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/LeeTNRASM22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Marta,
  author       = {Marta R. Costa{-}juss{\`{a}} and
                  James Cross and
                  Onur {\c{C}}elebi and
                  Maha Elbayad and
                  Kenneth Heafield and
                 },
  title        = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  journal      = {CoRR},
  volume       = {abs/2207.04672},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2207.04672},
  doi          = {10.48550/ARXIV.2207.04672},
  eprinttype    = {arXiv},
  eprint       = {2207.04672},
  timestamp    = {Tue, 13 Feb 2024 08:37:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2207-04672.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{OpenAI01,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/ARXIV.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 28 Aug 2023 21:26:19 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhangqi,
author={ ZhangQi and Gui Tao and Zheng Rui and Huang Xuanjing },
title={Large scale Language Models: From Theory to Practice},
journal={PUBLISHING HOUSE OF ELECTRONICS INDUSTRY},
year= {2023},
note = {Translated from Chinese}
}
@article{Teven,
  author       = {Teven Le Scao and
                  Angela Fan and
                  Christopher Akiki and
                  Ellie Pavlick and
                  Suzana Ilic and et al.},
  title        = {{BLOOM:} {A} 176B-Parameter Open-Access Multilingual Language Model},
  journal      = {CoRR},
  volume       = {abs/2211.05100},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2211.05100},
  doi          = {10.48550/ARXIV.2211.05100},
  eprinttype    = {arXiv},
  eprint       = {2211.05100},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2211-05100.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Ebtesam,
  author       = {Ebtesam Almazrouei and
                  Hamza Alobeidli and
                  Abdulaziz Alshamsi and
                  Alessandro Cappelli and
                  Ruxandra Cojocaru and
                  M{\'{e}}rouane Debbah and
                  {\'{E}}tienne Goffinet and
                  Daniel Hesslow and
                  Julien Launay and
                  Quentin Malartic and
                  Daniele Mazzotta and
                  Badreddine Noune and
                  Baptiste Pannier and
                  Guilherme Penedo},
  title        = {The Falcon Series of Open Language Models},
  journal      = {CoRR},
  volume       = {abs/2311.16867},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.16867},
  doi          = {10.48550/ARXIV.2311.16867},
  eprinttype    = {arXiv},
  eprint       = {2311.16867},
  timestamp    = {Mon, 04 Dec 2023 14:05:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-16867.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Aakanksh,
  author       = {Aakanksha Chowdhery and
                  Sharan Narang and
                  Jacob Devlin and
                  Maarten Bosma and
                  Gaurav Mishra and
                  Adam Roberts and
                  Paul Barham },
  title        = {PaLM: Scaling Language Modeling with Pathways},
  journal      = {J. Mach. Learn. Res.},
  volume       = {24},
  pages        = {240:1--240:113},
  year         = {2023},
  url          = {http://jmlr.org/papers/v24/22-1144.html},
  timestamp    = {Thu, 19 Oct 2023 09:44:46 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Hugo1,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.13971},
  doi          = {10.48550/ARXIV.2302.13971},
  eprinttype    = {arXiv},
  eprint       = {2302.13971},
  timestamp    = {Mon, 28 Aug 2023 21:26:20 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-13971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Hugo2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert  and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/ARXIV.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
  timestamp    = {Mon, 28 Aug 2023 21:26:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-09288.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Wenhao,
  author       = {Wenhao Zhu and
                  Hongyi Liu and
                  Qingxiu Dong and
                  Jingjing Xu and
                  Lingpeng Kong and
                  Jiajun Chen and
                  Lei Li and
                  Shujian Huang},
  title        = {Multilingual Machine Translation with Large Language Models: Empirical
                  Results and Analysis},
  journal      = {CoRR},
  volume       = {abs/2304.04675},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.04675},
  doi          = {10.48550/ARXIV.2304.04675},
  eprinttype    = {arXiv},
  eprint       = {2304.04675},
  timestamp    = {Tue, 22 Aug 2023 07:47:47 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-04675.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Yangjian,
  author       = {Yangjian Wu and
                  Gang Hu},
  editor       = {Philipp Koehn and
                  Barry Haddon and
                  Tom Kocmi and
                  Christof Monz},
  title        = {Exploring Prompt Engineering with {GPT} Language Models for Document-Level
                  Machine Translation: Insights and Findings},
  booktitle    = {Proceedings of the Eighth Conference on Machine Translation, {WMT}
                  2023, Singapore, December 6-7, 2023},
  pages        = {166--169},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.wmt-1.15},
  timestamp    = {Thu, 07 Dec 2023 13:54:50 +0100},
  biburl       = {https://dblp.org/rec/conf/wmt/WuH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Biao,
  author       = {Biao Zhang and
                  Barry Haddow and
                  Alexandra Birch},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Prompting Large Language Model for Machine Translation: {A} Case Study},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {41092--41110},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/zhang23m.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:09 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/0006HB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Wenxiang,
  author       = {Wenxiang Jiao and
                  Wenxuan Wang and
                  Jen{-}tse Huang and
                  Xing Wang and
                  Zhaopeng Tu},
  title        = {Is ChatGPT {A} Good Translator? {A} Preliminary Study},
  journal      = {CoRR},
  volume       = {abs/2301.08745},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2301.08745},
  doi          = {10.48550/ARXIV.2301.08745},
  eprinttype    = {arXiv},
  eprint       = {2301.08745},
  timestamp    = {Sun, 17 Dec 2023 20:56:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2301-08745.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Marjan,
  author       = {Marjan Ghazvininejad and
                  Hila Gonen and
                  Luke Zettlemoyer},
  title        = {Dictionary-based Phrase-level Prompting of Large Language Models for
                  Machine Translation},
  journal      = {CoRR},
  volume       = {abs/2302.07856},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.07856},
  doi          = {10.48550/ARXIV.2302.07856},
  eprinttype    = {arXiv},
  eprint       = {2302.07856},
  timestamp    = {Mon, 20 Feb 2023 14:27:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-07856.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Chunyou,
  author       = {Chunyou Li and
                  Mingtong Liu and
                  Hongxiao Zhang and
                  Yufeng Chen and
                  Jinan Xu and
                  Ming Zhou},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {{MT2:} Towards a Multi-Task Machine Translation Model with Translation-Specific
                  In-Context Learning},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {8616--8627},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.532},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/LiLZ0XZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{Amr,
  author       = {Amr Hendy and
                  Mohamed Abdelrehim and
                  Amr Sharaf and
                  Vikas Raunak and
                  Mohamed Gabr and
                  Hitokazu Matsushita and
                  Young Jin Kim and
                  Mohamed Afify and
                  Hany Hassan Awadalla},
  title        = {How Good Are {GPT} Models at Machine Translation? {A} Comprehensive
                  Evaluation},
  journal      = {CoRR},
  volume       = {abs/2302.09210},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.09210},
  doi          = {10.48550/ARXIV.2302.09210},
  eprinttype    = {arXiv},
  eprint       = {2302.09210},
  timestamp    = {Tue, 11 Jul 2023 17:14:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-09210.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Longyue,
  author       = {Longyue Wang and
                  Chenyang Lyu and
                  Tianbo Ji and
                  Zhirui Zhang and
                  Dian Yu and
                  Shuming Shi and
                  Zhaopeng Tu},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {Document-Level Machine Translation with Large Language Models},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {16646--16661},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.emnlp-main.1036},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/WangLJZY0T23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xu,
    title = "{INSTRUCTSCORE}: Towards Explainable Text Generation Evaluation with Automatic Feedback",
    author = "Xu, Wenda  and
      Wang, Danqing  and
      Pan, Liangming  and
      Song, Zhenqiao  and
      Freitag, Markus  and
      Wang, William  and
      Li, Lei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.365",
    doi = "10.18653/v1/2023.emnlp-main.365",
    pages = "5967--5994",
    abstract = "Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",
}

@inproceedings{Lianzhang,
  author       = {Lianzhang Lou and
                  Xi Yin and
                  Yutao Xie and
                  Yang Xiang},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {CCEval: {A} Representative Evaluation Benchmark for the Chinese-centric
                  Multilingual Machine Translation},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2023, Singapore, December 6-10, 2023},
  pages        = {10176--10184},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.findings-emnlp.682},
  timestamp    = {Wed, 13 Dec 2023 17:20:20 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/Lou0XX23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Keqin,
  author       = {Keqin Peng and
                  Liang Ding and
                  Qihuang Zhong and
                  Li Shen and
                  Xuebo Liu and
                  Min Zhang and
                  Yuanxin Ouyang and
                  Dacheng Tao},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {Towards Making the Most of ChatGPT for Machine Translation},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2023, Singapore, December 6-10, 2023},
  pages        = {5622--5633},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.findings-emnlp.373},
  timestamp    = {Sat, 27 Jan 2024 20:17:57 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/Peng0ZS0ZOT23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Nathaniel,
  author       = {Nathaniel R. Robinson and
                  Perez Ogayo and
                  David R. Mortensen and
                  Graham Neubig},
  editor       = {Philipp Koehn and
                  Barry Haddon and
                  Tom Kocmi and
                  Christof Monz},
  title        = {ChatGPT {MT:} Competitive for High- (but Not Low-) Resource Languages},
  booktitle    = {Proceedings of the Eighth Conference on Machine Translation, {WMT}
                  2023, Singapore, December 6-7, 2023},
  pages        = {392--418},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.wmt-1.40},
  timestamp    = {Thu, 07 Dec 2023 13:54:50 +0100},
  biburl       = {https://dblp.org/rec/conf/wmt/RobinsonOMN23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{jiao2023chatgpt,
      title={Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine}, 
      author={Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Xing Wang and Shuming Shi and Zhaopeng Tu},
      year={2023},
      eprint={2301.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{sanz2023google,
  title={Google Translate vs. ChatGPT: Can non-language professionals trust them for specialized translation?},
  author={Sanz-Valdivieso, Luc{\'\i}a and L{\'o}pez-Arroyo, Bel{\'e}n},
  booktitle={International Conference Human-informed Translation and Interpreting Technology (HiT-IT 2023)},
  pages={97--107},
  year={2023}
}

@inproceedings{Marzena,
  author       = {Marzena Karpinska and
                  Mohit Iyyer},
  editor       = {Philipp Koehn and
                  Barry Haddon and
                  Tom Kocmi and
                  Christof Monz},
  title        = {Large Language Models Effectively Leverage Document-level Context
                  for Literary Translation, but Critical Errors Persist},
  booktitle    = {Proceedings of the Eighth Conference on Machine Translation, {WMT}
                  2023, Singapore, December 6-7, 2023},
  pages        = {419--451},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.wmt-1.41},
  timestamp    = {Thu, 07 Dec 2023 13:54:50 +0100},
  biburl       = {https://dblp.org/rec/conf/wmt/KarpinskaI23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yang2023bigtranslate,
      title={BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages}, 
      author={Wen Yang and Chong Li and Jiajun Zhang and Chengqing Zong},
      year={2023},
      eprint={2305.18098},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2307-04408,
  author       = {Jiali Zeng and
                  Fandong Meng and
                  Yongjing Yin and
                  Jie Zhou},
  title        = {{TIM:} Teaching Large Language Models to Translate with Comparison},
  journal      = {CoRR},
  volume       = {abs/2307.04408},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.04408},
  doi          = {10.48550/ARXIV.2307.04408},
  eprinttype    = {arXiv},
  eprint       = {2307.04408},
  timestamp    = {Mon, 24 Jul 2023 16:32:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-04408.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2306-10968,
  author       = {Shaolei Zhang and
                  Qingkai Fang and
                  Zhuocheng Zhang and
                  Zhengrui Ma and
                  Yan Zhou and
                  Langlin Huang and
                  Mengyu Bu and
                  Shangtong Gui and
                  Yunji Chen and
                  Xilin Chen and
                  Yang Feng},
  title        = {BayLing: Bridging Cross-lingual Alignment and Instruction Following
                  through Interactive Translation for Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2306.10968},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.10968},
  doi          = {10.48550/ARXIV.2306.10968},
  eprinttype    = {arXiv},
  eprint       = {2306.10968},
  timestamp    = {Fri, 23 Jun 2023 15:19:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-10968.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2308-04948,
  author       = {Wenhao Zhu and
                  Yunzhe Lv and
                  Qingxiu Dong and
                  Fei Yuan and
                  Jingjing Xu and
                  Shujian Huang and
                  Lingpeng Kong and
                  Jiajun Chen and
                  Lei Li},
  title        = {Extrapolating Large Language Models to Non-English by Aligning Languages},
  journal      = {CoRR},
  volume       = {abs/2308.04948},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.04948},
  doi          = {10.48550/ARXIV.2308.04948},
  eprinttype    = {arXiv},
  eprint       = {2308.04948},
  timestamp    = {Tue, 22 Aug 2023 13:58:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-04948.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{DBLP:conf/nlpcc/XuWXH23,
  author       = {Haoyu Xu and
                  Xing Wang and
                  Xiaolin Xing and
                  Yu Hong},
  editor       = {Fei Liu and
                  Nan Duan and
                  Qingting Xu and
                  Yu Hong},
  title        = {Monolingual Denoising with Large Language Models for Low-Resource
                  Machine Translation},
  booktitle    = {Natural Language Processing and Chinese Computing - 12th National
                  {CCF} Conference, {NLPCC} 2023, Foshan, China, October 12-15, 2023,
                  Proceedings, Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14302},
  pages        = {413--425},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-44693-1\_33},
  doi          = {10.1007/978-3-031-44693-1\_33},
  timestamp    = {Wed, 11 Oct 2023 18:49:11 +0200},
  biburl       = {https://dblp.org/rec/conf/nlpcc/XuWXH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2309-11674,
  author       = {Haoran Xu and
                  Young Jin Kim and
                  Amr Sharaf and
                  Hany Hassan Awadalla},
  title        = {A Paradigm Shift in Machine Translation: Boosting Translation Performance
                  of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2309.11674},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.11674},
  doi          = {10.48550/ARXIV.2309.11674},
  eprinttype    = {arXiv},
  eprint       = {2309.11674},
  timestamp    = {Thu, 01 Feb 2024 14:14:44 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-11674.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/nips/Wei0SBIXCLZ22,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  editor       = {Sanmi Koyejo and
                  S. Mohamed and
                  A. Agarwal and
                  Danielle Belgrave and
                  K. Cho and
                  A. Oh},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {Advances in Neural Information Processing Systems 35: Annual Conference
                  on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
                  LA, USA, November 28 - December 9, 2022},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  timestamp    = {Mon, 08 Jan 2024 16:31:37 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Wei0SBIXCLZ22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/acl/ZhaoLJQB23,
  author       = {Ruochen Zhao and
                  Xingxuan Li and
                  Shafiq Joty and
                  Chengwei Qin and
                  Lidong Bing},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Verify-and-Edit: {A} Knowledge-Enhanced Chain-of-Thought Framework},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {5823--5840},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.acl-long.320},
  doi          = {10.18653/V1/2023.ACL-LONG.320},
  timestamp    = {Thu, 10 Aug 2023 12:35:51 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ZhaoLJQB23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2305-06575,
  author       = {Hongyuan Lu and
                  Haoyang Huang and
                  Dongdong Zhang and
                  Haoran Yang and
                  Wai Lam and
                  Furu Wei},
  title        = {Chain-of-Dictionary Prompting Elicits Translation in Large Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2305.06575},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.06575},
  doi          = {10.48550/ARXIV.2305.06575},
  eprinttype    = {arXiv},
  eprint       = {2305.06575},
  timestamp    = {Wed, 17 May 2023 15:47:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-06575.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2305-01181,
  author       = {Chenyang Lyu and
                  Jitao Xu and
                  Longyue Wang},
  title        = {New Trends in Machine Translation using Large Language Models: Case
                  Examples with ChatGPT},
  journal      = {CoRR},
  volume       = {abs/2305.01181},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.01181},
  doi          = {10.48550/ARXIV.2305.01181},
  eprinttype    = {arXiv},
  eprint       = {2305.01181},
  timestamp    = {Fri, 05 May 2023 14:35:02 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-01181.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/nlpcc/HuangWLWSWYZ23,
  author       = {Hui Huang and
                  Shuangzhi Wu and
                  Xinnian Liang and
                  Bing Wang and
                  Yanrui Shi and
                  Peihao Wu and
                  Muyun Yang and
                  Tiejun Zhao},
  editor       = {Fei Liu and
                  Nan Duan and
                  Qingting Xu and
                  Yu Hong},
  title        = {Towards Making the Most of {LLM} for Translation Quality Estimation},
  booktitle    = {Natural Language Processing and Chinese Computing - 12th National
                  {CCF} Conference, {NLPCC} 2023, Foshan, China, October 12-15, 2023,
                  Proceedings, Part {I}},
  series       = {Lecture Notes in Computer Science},
  volume       = {14302},
  pages        = {375--386},
  publisher    = {Springer},
  year         = {2023},
  url          = {https://doi.org/10.1007/978-3-031-44693-1\_30},
  doi          = {10.1007/978-3-031-44693-1\_30},
  timestamp    = {Mon, 05 Feb 2024 20:32:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nlpcc/HuangWLWSWYZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{fu2024reasonableness,
    author = {Fu, Tingchen and Liu, Lemao and Cai, Deng and Huang, Guoping and Shi, Shuming and Yan, Rui},
    title = {The Reasonableness Behind Unreasonable Translation Capability of Large Language Model},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year = {2024}
}
@inproceedings{DBLP:conf/eamt/BawdenY23,
  author       = {Rachel Bawden and
                  Fran{\c{c}}ois Yvon},
  editor       = {Mary Nurminen and
                  Judith Brenner and
                  Maarit Koponen and
                  Sirkku Latomaa and
                  Mikhail Mikhailov and
                  Frederike Schierl and
                  Tharindu Ranasinghe and
                  Eva Vanmassenhove and
                  Sergi Alvarez Vidal and
                  Nora Aranberri and
                  Mara Nunziatini and
                  Carla Parra Escart{\'{\i}}n and
                  Mikel L. Forcada and
                  Maja Popovic and
                  Carolina Scarton and
                  Helena Moniz},
  title        = {Investigating the Translation Performance of a Large Multilingual
                  Language Model: the Case of {BLOOM}},
  booktitle    = {Proceedings of the 24th Annual Conference of the European Association
                  for Machine Translation, {EAMT} 2023, Tampere, Finland, 12-15 June
                  2023},
  pages        = {157--170},
  publisher    = {European Association for Machine Translation},
  year         = {2023},
  url          = {https://aclanthology.org/2023.eamt-1.16},
  timestamp    = {Fri, 08 Sep 2023 15:18:30 +0200},
  biburl       = {https://dblp.org/rec/conf/eamt/BawdenY23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




%********************************************************************************************************************************

%********************************************************************************************************
@article{n4-1,
  title={JASS: Japanese-specific sequence to sequence pre-training for neural machine translation},
  author={Mao, Zhuoyuan and Cromieres, Fabien and Dabre, Raj and Song, Haiyue and Kurohashi, Sadao},
  journal={arXiv preprint arXiv:2005.03361},
  year={2020}
}
@article{n4-2,
  title={Dynamic curriculum learning for low-resource neural machine translation},
  author={Xu, Chen and Hu, Bojie and Jiang, Yufan and Feng, Kai and Wang, Zeyang and Huang, Shen and Ju, Qi and Xiao, Tong and Zhu, Jingbo},
  journal={arXiv preprint arXiv:2011.14608},
  year={2020}
}

@article{DBLP:journals/corr/abs-2109-00486,
  author       = {Barry Haddow and
                  Rachel Bawden and
                  Antonio Valerio Miceli Barone and
                  Jindrich Helcl and
                  Alexandra Birch},
  title        = {Survey of Low-Resource Machine Translation},
  journal      = {CoRR},
  volume       = {abs/2109.00486},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.00486},
  eprinttype    = {arXiv},
  eprint       = {2109.00486},
  timestamp    = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-00486.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2106-15115,
  author       = {Surangika Ranathunga and
                  En{-}Shiun Annie Lee and
                  Marjana Prifti Skenduli and
                  Ravi Shekhar and
                  Mehreen Alam and
                  Rishemjit Kaur},
  title        = {Neural Machine Translation for Low-Resource Languages: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2106.15115},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.15115},
  eprinttype    = {arXiv},
  eprint       = {2106.15115},
  timestamp    = {Mon, 05 Jul 2021 15:15:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-15115.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{n4-3,
  title={Dynamic data selection and weighting for iterative back-translation},
  author={Dou, Zi-Yi and Anastasopoulos, Antonios and Neubig, Graham},
  journal={arXiv preprint arXiv:2004.03672},
  year={2020}
}
@article{n4-4,
  title={Optimizing transformer for low-resource neural machine translation},
  author={Araabi, Ali and Monz, Christof},
  journal={arXiv preprint arXiv:2011.02266},
  year={2020}
}
@article{n4-5,
  title={Improving multilingual neural machine translation for low-resource languages: French, English-Vietnamese},
  author={Ngo, Thi-Vinh and Nguyen, Phuong-Thai and Ha, Thanh-Le and Dinh, Khac-Quy and Nguyen, Le-Minh},
  journal={arXiv preprint arXiv:2012.08743},
  year={2020}
}
@misc{n4-one1,
      title={Generalized Data Augmentation for Low-Resource Translation}, 
      author={Mengzhou Xia and Xiang Kong and Antonios Anastasopoulos and Graham Neubig},
      year={2019},
      eprint={1906.03785},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{n4-four4,
  title={Cosda-ml: Multi-lingual code-switching data augmentation for zero-shot cross-lingual nlp},
  author={Qin, Libo and Ni, Minheng and Zhang, Yue and Che, Wanxiang},
  journal={arXiv preprint arXiv:2006.06402},
  year={2020}
}
@article{n4-seven7,
  title={Data diversification: A simple strategy for neural machine translation},
  author={Nguyen, Xuan-Phi and Joty, Shafiq and Wu, Kui and Aw, Ai Ti},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10018--10029},
  year={2020}
}
@article{n4-eight8,
  title={Improving neural machine translation robustness via data augmentation: Beyond back translation},
  author={Li, Zhenhao and Specia, Lucia},
  journal={arXiv preprint arXiv:1910.03009},
  year={2019}
}
@misc{n4-nine9,
  title={Valar nmt: Vastly lacking resources neural machine translation},
  author={Kang, Min-Hyung and Kudrolli, Kais},
  year={2019},
  publisher={Stanford CS224N}
}
@inproceedings{n4-ten10,
  title={Lexical-constraint-aware neural machine translation via data augmentation},
  author={Chen, Guanhua and Chen, Yun and Wang, Yong and Li, Victor OK},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={3587--3593},
  year={2021}
}
@article{n4-eleven11,
  title={Syntax-aware data augmentation for neural machine translation},
  author={Duan, Sufeng and Zhao, Hai and Zhang, Dongdong and Wang, Rui},
  journal={arXiv preprint arXiv:2004.14200},
  year={2020}
}
@inproceedings{n4-6,
  title={Domain adaptation via pseudo in-domain data selection},
  author={Axelrod, Amittai and He, Xiaodong and Gao, Jianfeng},
  booktitle={Proceedings of the 2011 conference on empirical methods in natural language processing},
  pages={355--362},
  year={2011}
}
@article{n4-7,
  title={Dynamic data selection for neural machine translation},
  author={Van Der Wees, Marlies and Bisazza, Arianna and Monz, Christof},
  journal={arXiv preprint arXiv:1708.00712},
  year={2017}
}
@inproceedings{n4-8,
  title={Active learning for neural machine translation},
  author={Zhang, Pei and Xu, Xueying and Xiong, Deyi},
  booktitle={2018 International Conference on Asian Language Processing (IALP)},
  pages={153--158},
  year={2018},
  organization={IEEE}
}
@article{n4-9,
  title={Sentence selection and weighting for neural machine translation domain adaptation},
  author={Wang, Rui and Utiyama, Masao and Finch, Andrew and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={10},
  pages={1727--1741},
  year={2018},
  publisher={IEEE}
}
@article{n4-10,
  title={Code-switching for enhancing NMT with pre-specified translation},
  author={Song, Kai and Zhang, Yue and Yu, Heng and Luo, Weihua and Wang, Kun and Zhang, Min},
  journal={arXiv preprint arXiv:1904.09107},
  year={2019}
}
@article{n4-11,
  title={The annotation technology of Japanese scientific language in the Sino Japanese bilingual parallel corpus},
  author={Yipeng Li},
  journal={Guide to Business},
  number={2},
  pages={175--176},
  year={2015},
}

@Article{n4-tow2,
AUTHOR = {Li, Yu and Li, Xiao and Yang, Yating and Dong, Rui},
TITLE = {A Diverse Data Augmentation Strategy for Low-Resource Neural Machine Translation},
JOURNAL = {Information},
VOLUME = {11},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {255},
URL = {https://www.mdpi.com/2078-2489/11/5/255},
ISSN = {2078-2489},
DOI = {10.3390/info11050255}
}
@article{n4-five5,
  title={SwitchOut: an efficient data augmentation algorithm for neural machine translation},
  author={Wang, Xinyi and Pham, Hieu and Dai, Zihang and Neubig, Graham},
  journal={arXiv preprint arXiv:1808.07512},
  year={2018}
}
@inproceedings{n4-six6,
  title={Soft contextual data augmentation for neural machine translation},
  author={Gao, Fei and Zhu, Jinhua and Wu, Lijun and Xia, Yingce and Qin, Tao and Cheng, Xueqi and Zhou, Wengang and Liu, Tie-Yan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5539--5544},
  year={2019}
}
@misc{n4-nineteen19,
      title={Generalizing Back-Translation in Neural Machine Translation}, 
      author={Miguel Graça and Yunsu Kim and Julian Schamper and Shahram Khadivi and Hermann Ney},
      year={2019},
      eprint={1906.07286},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{n4-twenty20,
  title={Multi-source neural machine translation with data augmentation},
  author={Nishimura, Yuta and Sudoh, Katsuhito and Neubig, Graham and Nakamura, Satoshi},
  journal={arXiv preprint arXiv:1810.06826},
  year={2018}
}
@inproceedings{n4-twentyone21,
  title={Data augmentation using back-translation for context-aware neural machine translation},
  author={Sugiyama, Amane and Yoshinaga, Naoki},
  booktitle={Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)},
  pages={35--44},
  year={2019}
}
@article{n4-13,
  title={Tagged back-translation},
  author={Caswell, Isaac and Chelba, Ciprian and Grangier, David},
  journal={arXiv preprint arXiv:1906.06442},
  year={2019}
}
@inproceedings{n4-14,
  title={Filtering back-translated data in unsupervised neural machine translation},
  author={Khatri, Jyotsana and Bhattacharyya, Pushpak},
  booktitle={Proceedings of the 28th international conference on computational linguistics},
  pages={4334--4339},
  year={2020}
}
@article{n4-15,
  title={Iterative domain-repaired back-translation},
  author={Wei, Haoran and Zhang, Zhirui and Chen, Boxing and Luo, Weihua},
  journal={arXiv preprint arXiv:2010.02473},
  year={2020}
}
@article{n4-16,
  author       = {Idris Abdulmumin and
                  Bashir Shehu Galadanci and
                  Ismaila Idris Sinan},
  title        = {Iterative Self-Learning for Enhanced Back-Translation in Low Resource
                  Neural Machine Translation},
  journal      = {CoRR},
  volume       = {abs/2011.07403},
  year         = {2020},
  url          = {https://arxiv.org/abs/2011.07403},
  eprinttype    = {arXiv},
  eprint       = {2011.07403},
  timestamp    = {Wed, 18 Nov 2020 16:48:35 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2011-07403.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{n4-17,
  title={Meta back-translation},
  author={Pham, Hieu and Wang, Xinyi and Yang, Yiming and Neubig, Graham},
  journal={arXiv preprint arXiv:2102.07847},
  year={2021}
}
@article{n4-18,
  title={A Chinese-Vietnamese neural machine translation method based on synonym data augmentation},
  author={You, Congcong and Gao, Shengxiang and Yu, Zhengtao and Mao, Cunli and Pan, Runhai},
  journal={Computer Engineering \& Science},
  volume={43},
  number={08},
  pages={1497},
  year={2021}
}
@article{n4-19,
author = {Jia,Chengxun and Lai,Hua and Yu,Zhengtao and Wen,Yonghua and Yu,Zhiqiang},
title = {Phrase Substitution Based Pseudo-parallel Sentence Pair Generation Between Chinese and Vietnamese},
publisher = {Journal of Chinese Information Processing},
year = {2021},
journal = {Journal of Chinese Information Processing},
volume = {35},
number = {8},
eid = {47},
pages = {47-55},
keywords = {Chinese-Vietnamese neural machine translation； data augmentation； pseudo-parallel sentence pairs； phrase alignment table； phrase substitute},
doi = {},
}
@article{n4-20,
  title={Cooperative Research on Chinese-Japanese Machine Translation for S\&T Documents},
  author={Zhao,Zhiyun and Shi,Chongde and He,Yanqing and Gao,Yingfan and Yao,Changqing},
  journal={Technology Intelligence Engineering},
  volume={3},
  number={3},
  pages={4--9},
  year={2017}
}
@inproceedings{n4-21,
  title={LIT Team’s System Description for Japanese-Chinese Machine Translation Task in IWSLT 2020},
  author={Zhuang, Yimeng and Zhang, Yuan and Wang, Lijie},
  booktitle={Proceedings of the 17th International Conference on Spoken Language Translation},
  pages={109--113},
  year={2020}
}
@inproceedings{n4-22,
  title={Octanove Labs’ Japanese-Chinese Open Domain Translation System},
  author={Hagiwara, Masato},
  booktitle={Proceedings of the 17th International Conference on Spoken Language Translation},
  pages={166--171},
  year={2020}
}
@article{n4-twelve12,
  title={Improving robustness of machine translation with synthetic noise},
  author={Vaibhav, Vaibhav and Singh, Sumeet and Stewart, Craig and Neubig, Graham},
  journal={arXiv preprint arXiv:1902.09508},
  year={2019}
}
@article{n4-thirteen13,
  title={Synthetic data for neural machine translation of spoken-dialects},
  author={Hassan, Hany and Elaraby, Mostafa and Tawfik, Ahmed},
  journal={arXiv preprint arXiv:1707.00079},
  year={2017}
}
@inproceedings{n4-fourteen14,
  title={Neural fuzzy repair: Integrating fuzzy matches into neural machine translation},
  author={Bult{\'e}, Bram and Tezcan, Arda},
  booktitle={57th Annual Meeting of the Association-for-Computational-Linguistics (ACL)},
  pages={1800--1809},
  year={2019}
}
@article{n4-fifteen15,
  title={Dictionary-based data augmentation for cross-domain neural machine translation},
  author={Peng, Wei and Huang, Chongxuan and Li, Tianhao and Chen, Yun and Liu, Qun},
  journal={arXiv preprint arXiv:2004.02577},
  year={2020}
}
@inproceedings{n4-sixteen16,
  title={Sentence boundary augmentation for neural machine translation robustness},
  author={Li, Daniel and Te, I and Arivazhagan, Naveen and Cherry, Colin and Padfield, Dirk},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7553--7557},
  year={2021},
  organization={IEEE}
}
@inproceedings{n4-seventeen17,
  title={Adapting neural machine translation with parallel synthetic data},
  author={Chinea-Rios, Mara and Peris, Alvaro and Casacuberta, Francisco},
  booktitle={Proceedings of the Second Conference on Machine Translation},
  pages={138--147},
  year={2017}
}
@article{n4-eighteen18,
  title={A diverse data augmentation strategy for low-resource neural machine translation},
  author={Li, Yu and Li, Xiao and Yang, Yating and Dong, Rui},
  journal={Information},
  volume={11},
  number={5},
  pages={255},
  year={2020},
  publisher={MDPI}
}
@article{n4-23,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}
@article{n4-24,
  title={Explicit sparse transformer: Concentrated attention through explicit selection},
  author={Zhao, Guangxiang and Lin, Junyang and Zhang, Zhiyuan and Ren, Xuancheng and Su, Qi and Sun, Xu},
  journal={arXiv preprint arXiv:1912.11637},
  year={2019}
}
@article{n4-25,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{n4-26,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17283--17297},
  year={2020}
}
@inproceedings{n4-27,
  title={Informer: Beyond efficient transformer for long sequence time-series forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={12},
  pages={11106--11115},
  year={2021}
}
@inproceedings{n4-28,
  title={Improved English to Russian translation by neural suffix prediction},
  author={Song, Kai and Zhang, Yue and Zhang, Min and Luo, Weihua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}
@inproceedings{n4-29,
  title={BiTIIMT: a bilingual text-infilling method for interactive machine translation},
  author={Xiao, Yanling and Liu, Lemao and Huang, Guoping and Cui, Qu and Huang, Shujian and Shi, Shuming and Chen, Jiajun},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1958--1969},
  year={2022}
}
@article{n4-30,
  title={Deepnet: Scaling transformers to 1,000 layers},
  author={Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Wei, Furu},
  journal={arXiv preprint arXiv:2203.00555},
  year={2022}
}
@inproceedings{n4-31,
  title={Frequency-aware contrastive learning for neural machine translation},
  author={Zhang, Tong and Ye, Wei and Yang, Baosong and Zhang, Long and Ren, Xingzhang and Liu, Dayiheng and Sun, Jinan and Zhang, Shikun and Zhang, Haibo and Zhao, Wen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11712--11720},
  year={2022}
}

@article{DBLP:journals/corr/abs-2204-04344,
  author       = {Bin Li and
                  Yixuan Weng and
                  Fei Xia and
                  Hanjun Deng},
  title        = {Towards Better Chinese-centric Neural Machine Translation for Low-resource
                  Languages},
  journal      = {CoRR},
  volume       = {abs/2204.04344},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2204.04344},
  doi          = {10.48550/ARXIV.2204.04344},
  eprinttype    = {arXiv},
  eprint       = {2204.04344},
  timestamp    = {Wed, 10 May 2023 21:51:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2204-04344.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{n4-32,
  title={Non-parametric online learning from human feedback for neural machine translation},
  author={Wang, Dongqi and Wei, Haoran and Zhang, Zhirui and Huang, Shujian and Xie, Jun and Chen, Jiajun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={11431--11439},
  year={2022}
}
@article{n4-33,
  title={Is word segmentation necessary for deep learning of Chinese representations?},
  author={Li, Xiaoya and Meng, Yuxian and Sun, Xiaofei and Han, Qinghong and Yuan, Arianna and Li, Jiwei},
  journal={arXiv preprint arXiv:1905.05526},
  year={2019}
}
@article{n4-34,
  title={Revisiting character-based neural machine translation with capacity and compression},
  author={Cherry, Colin and Foster, George and Bapna, Ankur and Firat, Orhan and Macherey, Wolfgang},
  journal={arXiv preprint arXiv:1808.09943},
  year={2018}
}
@article{n4-35,
  title={On the importance of word boundaries in character-level neural machine translation},
  author={Ataman, Duygu and Firat, Orhan and Di Gangi, Mattia A and Federico, Marcello and Birch, Alexandra},
  journal={arXiv preprint arXiv:1910.06753},
  year={2019}
}
@inproceedings{n4-36,
  title={Combining character and word information in neural machine translation using a multi-level attention},
  author={Chen, Huadong and Huang, Shujian and Chiang, David and Dai, Xinyu and Chen, Jiajun},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={1284--1293},
  year={2018}
}
@article{n4-37,
  title={Subcharacter chinese-english neural machine translation with Wubi encoding},
  author={Zhang, Wei and Lin, Feifei and Wang, Xiaodong and Liang, Zhenshuang and Huang, Zhen},
  journal={arXiv preprint arXiv:1911.02737},
  year={2019}
}
@inproceedings{n4-38,
  title={Incorporating source syntax into transformer-based neural machine translation},
  author={Currey, Anna and Heafield, Kenneth},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)},
  pages={24--33},
  year={2019}
}
@article{n4-39,
  title={Character-level translation with self-attention},
  author={Gao, Yingqiang and Nikolov, Nikola I and Hu, Yuhuang and Hahnloser, Richard HR},
  journal={arXiv preprint arXiv:2004.14788},
  year={2020}
}
@inproceedings{n4-40,
  title={Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance},
  author={Ma, Qingsong and Bojar, Ond{\v{r}}ej and Graham, Yvette},
  booktitle={Proceedings of the third conference on machine translation: shared task papers},
  pages={671--688},
  year={2018}
}
@article{n4-41,
  title={Analyzing the problems of vocabulary in Japanese-Chinese neural network machine translation},
  author={Luo, Wentao},
  journal={Computer Science and Application},
  volume={10},
  number={3},
  pages={387--397},
  year={2020}
}
@inproceedings{n4-42,
  title={On the word alignment from neural machine translation},
  author={Li, Xintong and Li, Guanlin and Liu, Lemao and Meng, Max and Shi, Shuming},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1293--1303},
  year={2019}
}
@article{n4-43,
  title={Translation Quality Estimation of Chinese-Vietnamese Neural Machine Translation Incorporating Linguistic Differentiation Features},
  author={Zou,Xiang and Zhu,Junguo and Gao,Shengxiang and Yu,Zhengtao and Yang,Fuan},
  journal={Journal of Chinese Computer Systems},
  volume={43},
  number={7},
  pages={1413--1418},
  year={2022},
  url={10.20009/j.cnki.21-1106/TP.2020-1084}
}
@inproceedings{n4-44,
  title={Alignment-enhanced transformer for constraining nmt with pre-specified translations},
  author={Song, Kai and Wang, Kun and Yu, Heng and Zhang, Yue and Huang, Zhongqiang and Luo, Weihua and Duan, Xiangyu and Zhang, Min},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8886--8893},
  year={2020}
}
@article{n4-45,
  title={Linguistic input features improve neural machine translation},
  author={Sennrich, Rico and Haddow, Barry},
  journal={arXiv preprint arXiv:1606.02892},
  year={2016}
}
@inproceedings{n4-46,
  title={Neural machine translation with source dependency representation},
  author={Chen, Kehai and Wang, Rui and Utiyama, Masao and Liu, Lemao and Tamura, Akihiro and Sumita, Eiichiro and Zhao, Tiejun},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={2846--2852},
  year={2017}
}
@article{n4-47,
  title={Tree-to-sequence attentional neural machine translation},
  author={Eriguchi, Akiko and Hashimoto, Kazuma and Tsuruoka, Yoshimasa},
  journal={arXiv preprint arXiv:1603.06075},
  year={2016}
}

@article{n4-48,
  title={Improved neural machine translation with a syntax-aware encoder and decoder},
  author={Chen, Huadong and Huang, Shujian and Chiang, David and Chen, Jiajun},
  journal={arXiv preprint arXiv:1707.05436},
  year={2017}
}

@article{n4-49,
  title={Top-down tree structured decoding with syntactic connections for neural machine translation and parsing},
  author={G{\=u}, Jetic and Shavarani, Hassan S and Sarkar, Anoop},
  journal={arXiv preprint arXiv:1809.01854},
  year={2018}
}
@article{n4-50,
  title={Promoting the knowledge of source syntax in transformer nmt is not needed},
  author={Pham, Thuong-Hai and Mach{\'a}{\v{c}}ek, Dominik and Bojar, Ond{\v{r}}ej},
  journal={Computaci{\'o}n y Sistemas},
  volume={23},
  number={3},
  pages={923--934},
  year={2019},
  publisher={Instituto Polit{\'e}cnico Nacional, Centro de Investigaci{\'o}n en Computaci{\'o}n}
}
@article{n4-51,
  title={Encodings of source syntax: Similarities in NMT representations across target languages},
  author={Chang, Tyler A and Rafferty, Anna N},
  journal={arXiv preprint arXiv:2005.08177},
  year={2020}
}
@article{n4-52,
  title={Syntax-enhanced neural machine translation with syntax-aware word representations},
  author={Zhang, Meishan and Li, Zhenghua and Fu, Guohong and Zhang, Min},
  journal={arXiv preprint arXiv:1905.02878},
  year={2019}
}
@article{n4-53,
  title={Tree transformer: Integrating tree structures into self-attention},
  author={Wang, Yau-Shian and Lee, Hung-Yi and Chen, Yun-Nung},
  journal={arXiv preprint arXiv:1909.06639},
  year={2019}
}
@article{n4-55,
  title={Language model prior for low-resource neural machine translation},
  author={Baziotis, Christos and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:2004.14928},
  year={2020}
}
@article{n4-56,
  title={Augmenting neural machine translation with knowledge graphs},
  author={Moussallem, Diego and Ar{\v{c}}an, Mihael and Ngomo, Axel-Cyrille Ngonga and Buitelaar, Paul},
  journal={arXiv preprint arXiv:1902.08816},
  year={2019}
}


@article{DBLP:journals/corr/abs-2311-08348,
  author       = {Chen Zhang and
                  Mingxu Tao and
                  Quzhe Huang and
                  Jiuheng Lin and
                  Zhibin Chen and
                  Yansong Feng},
  title        = {MC{\^{}}2: {A} Multilingual Corpus of Minority Languages in China},
  journal      = {CoRR},
  volume       = {abs/2311.08348},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2311.08348},
  doi          = {10.48550/ARXIV.2311.08348},
  eprinttype    = {arXiv},
  eprint       = {2311.08348},
  timestamp    = {Tue, 21 Nov 2023 13:55:21 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2311-08348.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{n4-57,
  title={Multi-round transfer learning for low-resource NMT using multiple high-resource languages},
  author={Maimaiti, Mieradilijiang and Liu, Yang and Luan, Huanbo and Sun, Maosong},
  journal={ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},
  volume={18},
  number={4},
  pages={1--26},
  year={2019},
  publisher={ACM New York, NY, USA}
}
@article{n4-58,
  title={Hierarchical transfer learning architecture for low-resource neural machine translation},
  author={Luo, Gongxu and Yang, Yating and Yuan, Yang and Chen, Zhanheng and Ainiwaer, Aizimaiti},
  journal={IEEE Access},
  volume={7},
  pages={154157--154166},
  year={2019},
  publisher={IEEE}
}

@article{n4-59,
  title={Pivot-based transfer learning for neural machine translation between non-English languages},
  author={Kim, Yunsu and Petrov, Petre and Petrushkov, Pavel and Khadivi, Shahram and Ney, Hermann},
  journal={arXiv preprint arXiv:1909.09524},
  year={2019}
}
@inproceedings{n4-60,
  title={In neural machine translation, what does transfer learning transfer?},
  author={Aji, Alham Fikri and Bogoychev, Nikolay and Heafield, Kenneth and Sennrich, Rico},
  year={2020},
  organization={Association for Computational Linguistics}
}
@inproceedings{n4-61,
  title={Exploiting multilingualism through multistage fine-tuning for low-resource neural machine translation},
  author={Dabre, Raj and Fujita, Atsushi and Chu, Chenhui},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={1410--1416},
  year={2019}
}
@article{n4-62,
  title={Enriching the transfer learning with pre-trained lexicon embedding for low-resource neural machine translation},
  author={Maimaiti, Mieradilijiang and Liu, Yang and Luan, Huanbo and Sun, Maosong},
  journal={Tsinghua Science and Technology},
  volume={27},
  number={1},
  pages={150--163},
  year={2021},
  publisher={TUP}
}
@article{n4-63,
  title={Exploiting out-of-domain parallel data through multilingual transfer learning for low-resource neural machine translation},
  author={Imankulova, Aizhan and Dabre, Raj and Fujita, Atsushi and Imamura, Kenji},
  journal={arXiv preprint arXiv:1907.03060},
  year={2019}
}
@inproceedings{n4-64,
  title={Cross-lingual pre-training based transfer for zero-shot neural machine translation},
  author={Ji, Baijun and Zhang, Zhirui and Duan, Xiangyu and Zhang, Min and Chen, Boxing and Luo, Weihua},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={01},
  pages={115--122},
  year={2020}
}
@inproceedings{n4-65,
  title={Taking Actions Separately: A Bidirectionally-Adaptive Transfer Learning Method for Low-Resource Neural Machine Translation},
  author={Xing, Xiaolin and Hong, Yu and Xu, Minhan and Yao, Jianmin and Zhou, Guodong},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={4481--4491},
  year={2022}
}
@article{n4-66,
  title={ConsistTL: Modeling Consistency in Transfer Learning for Low-Resource Neural Machine Translation},
  author={Li, Zhaocong and Liu, Xuebo and Wong, Derek F and Chao, Lidia S and Zhang, Min},
  journal={arXiv preprint arXiv:2212.04262},
  year={2022}
}
@article{n4-67,
  title={A joint back-translation and transfer learning method for low-resource neural machine translation},
  author={Luo, Gong-Xu and Yang, Ya-Ting and Dong, Rui and Chen, Yan-Hong and Zhang, Wen-Bo},
  journal={Mathematical Problems in Engineering},
  volume={2020},
  pages={1--11},
  year={2020},
  publisher={Hindawi Limited}
}
@inproceedings{n4-68,
  title={Efficient neural machine translation for low-resource languages via exploiting related languages},
  author={Goyal, Vikrant and Kumar, Sourav and Sharma, Dipti Misra},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics: student research workshop},
  pages={162--168},
  year={2020}
}
@inproceedings{n4-69,
  title={Low-resource unsupervised NMT: Diagnosing the problem and providing a linguistically motivated solution},
  author={Edman, Lukas and Toral, Antonio and van Noord, Gertjan},
  booktitle={Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
  pages={81--90},
  year={2020}
}
@article{n4-70,
  author       = {Alexandra Chronopoulou and
                  Dario Stojanovski and
                  Alexander M. Fraser},
  title        = {Reusing a Pretrained Language Model on Languages with Limited Corpora
                  for Unsupervised {NMT}},
  journal      = {CoRR},
  volume       = {abs/2009.07610},
  year         = {2020}
}
@article{n4-71,
  title={On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss},
  author={Liu, Yihong and Chronopoulou, Alexandra and Sch{\"u}tze, Hinrich and Fraser, Alexander},
  journal={arXiv preprint arXiv:2305.17182},
  year={2023}
}
@inproceedings{n4-72,
    title = "Unsupervised Neural Machine Translation for {E}nglish and {M}anipuri",
    author = "Singh, Salam Michael  and
      Singh, Thoudam Doren",
    booktitle = "Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages",
    month = dec,
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.loresmt-1.10",
pages = "69--78",
year = {2020}
}
@article{n4-73,
  author       = {Haipeng Sun and
                  Rui Wang and
                  Kehai Chen and
                  Masao Utiyama and
                  Eiichiro Sumita and
                  Tiejun Zhao},
  title        = {Knowledge Distillation for Multilingual Unsupervised Neural Machine
                  Translation},
  journal      = {CoRR},
  volume       = {abs/2004.10171},
  year         = {2020}
}
@article{n4-74,
author       = {Jiawei Wu and                   Xin Wang and                   William Yang Wang},   title        = {Extract and Edit: An Alternative to Back-Translation for Unsupervised                   Neural Machine Translation},   journal      = {CoRR},   volume       = {abs/1904.02331},   year         = {2019} }

@article{n4-75,
author       = {Zuchao Li and                   Hai Zhao and                   Rui Wang and                   Masao Utiyama and                   Eiichiro Sumita},   title        = {Reference Language based Unsupervised Neural Machine Translation},   journal      = {CoRR},   volume       = {abs/2004.02127},   year         = {2020} }
@article{n4-a1,
  title={Phrase-based \& neural unsupervised machine translation},
  author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1804.07755},
  year={2018}
}

@article{n4-a2,
  title={Word translation without parallel data},
  author={Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1710.04087},
  year={2017}
}
@article{n4-a3,
  title={Unsupervised machine translation using monolingual corpora only},
  author={Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1711.00043},
  year={2017}
}
@article{n4-76,
author       = {Zi{-}Yi Dou and                   Junjie Hu and                   Antonios Anastasopoulos and                   Graham Neubig},   title        = {Unsupervised Domain Adaptation for Neural Machine Translation with                   Domain-Aware Feature Embeddings},   journal      = {CoRR},   volume       = {abs/1908.10430},   year         = {2019} }

@article{n4-77,
author       = {Po{-}Yao Huang and                   Junjie Hu and                   Xiaojun Chang and                   Alexander G. Hauptmann},   title        = {Unsupervised Multimodal Neural Machine Translation with Pseudo Visual                   Pivoting},   journal      = {CoRR},   volume       = {abs/2005.03119},   year         = {2020} }

@inproceedings{n4-78,
  title={Polygon-Net: A General Framework for Jointly Boosting Multiple Unsupervised Neural Machine Translation Models},
  author={Xu, Chang and Qin, Tao and Wang, Gang and Liu, Tie-Yan},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2019},
  pages={5320-5326}
}
@article{n4-79,
author       = {Sreyashi Nag and                   Mihir Kale and                   Varun Lakshminarasimhan and                   Swapnil Singhavi},   title        = {Incorporating Bilingual Dictionaries for Low Resource Semi-Supervised                   Neural Machine Translation},   journal      = {CoRR},   volume       = {abs/2004.02071},   year         = {2020} }
@Inbook{n4-80,
author="Cheng, Yong",
title="Semi-supervised Learning for Neural Machine Translation",
bookTitle="Joint Training for Neural Machine Translation",
year="2019",
publisher="Springer Singapore",
address="Singapore",
pages="25--40",
isbn="978-981-32-9748-7",
}
@article{n4-81,
author       = {Weijia Xu and Xing Niu and                   Marine Carpuat},   title        = {Dual Reconstruction: a Unifying Objective for Semi-Supervised Neural                   Machine Translation},   journal      = {CoRR},   volume       = {abs/2010.03412},   year         = {2020} }
@Article{n4-82,
AUTHOR = {Zhang, Wenbo and Li, Xiao and Yang, Yating and Dong, Rui and Luo, Gongxu},
TITLE = {Keeping Models Consistent between Pretraining and Translation for Low-Resource Neural Machine Translation},
JOURNAL = {Future Internet},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {215},
URL = {https://www.mdpi.com/1999-5903/12/12/215},
ISSN = {1999-5903},
DOI = {10.3390/fi12120215}
}
@ARTICLE{n4-83,
  author={Wang, Yijun and Xia, Yingce and Zhao, Li and Bian, Jiang and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Semi-Supervised Neural Machine Translation via Marginal Distribution Estimation}, 
  year={2019},
  volume={27},
  number={10},
  pages={1564-1576},
  doi={10.1109/TASLP.2019.2921423}}
@article{n4-84,
  title={When and why is unsupervised neural machine translation useless?},
  author={Kim, Yunsu and Gra{\c{c}}a, Miguel and Ney, Hermann},
  journal={arXiv preprint arXiv:2004.10581},
  year={2020}
}

%********************************************************************************************************

@article{4-2,
 author = {HE,Wuyun and XIU,Zhi and BAO,Jingjing and CHEN,Meilan and WANG,Siriguleng},
 title = {Mongolian-Chinese neural machine translation system based on word segmentation with BERT data enhancement},
 journal = {Journal of Xiamen University(Natural Science)},
 volume = {61},
 number = {667-674},
 year = {2022},
 issn = {0438-0479},
 doi ={}
 }

@inproceedings{4-2b1,
  title={Exploiting source-side monolingual data in neural machine translation},
  author={Zhang, Jiajun and Zong, Chengqing},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={1535--1545},
  year={2016}
}
@mastersthesis{4-2b2,
 author = {Bai,Tiangang},
 title = {Mongolian-Chinese Neural Machine Translation Based on Reinforcement Learning},
 school = {Inner Mongolia University},
 year = {2020}
 }
@article{4-2b3,
 author = {Hou,Hongxu and Sun,Shuo and Wu,Nier},
 title = {Survey of Mongolian-Chinese Neural Machine Translation},
 journal = {Computer Science},
 volume = {49},
 number = {31-40},
 year = {2022},
 issn = {1002-137X},
 doi ={}
 }

@InProceedings{10.1007/978-981-99-7894-69,
author="Guo, Shuao
and Deng, Ningyuan
and He, Yanqing",
editor="Feng, Yang
and Feng, Chong",
title="ISTIC's Neural Machine Translation Systems for CCMT' 2023",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="94--102",
isbn="978-981-99-7894-6"
}

@phdthesis{4-2b4,
 author = {Mir Adili Jiang Maimaiti},
 title = {Research on Neural Machine Translation Methods under Low-Resource Conditions},
 school = {Tsinghua University},
 year = {2021}
 }
@article{4-2b5,
  title={Data augmentation for low-resource neural machine translation},
  author={Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof},
  journal={arXiv preprint arXiv:1705.00440},
  year={2017}
}


@article{4-1,
  title={Research on Multi-granularity Mongolian-Chinese Neural Network Machine Translation Based on Multi-Granularity Neural Network Model},
  author={Wang, Haibo},
  journal={Journal of Inner Mongolia University (Natural Science Edition)},
  year={2018},
  volume={49},
  number={5},
  pages={590--597}
}
@article{4-3,
 author = { HE,Wuyun and WANG,Siriguleng},
 title = {Application of Neural Network Word Slicing Method in Mongolian-Chinese Machine Translation},
 journal = {Journal of Minzu University of China(Natural Sciences Edition)},
 volume = {31},
 number = {36-46},
 year = {2022},
 issn = {1005-8036},
 doi ={}
 }
@article{4-4,
 author = {Su,Yila and Gao,Fen and Niu,Xianghua and Ren,Qingdaoerji},
 title = {PRE-TRAINING CROSS MONGOLIAN-CHINESE LANGUAGE MODEL BASED ON SELF-ATTENTION MECHANISM},
 journal = {Computer Applications and Software},
 volume = {38},
 number = {165-170},
 year = {2021},
 issn = {1000-386X},
 doi ={}
 }
@article{4-5,
 author = {Wang,Yufei and Su,Yila and Zhao,Yaping and Sun,Xiaoqian and Ren,Qingdaoerji},
 title = {Mongolian-Chinese Neural Machine Translation Model Based On Parameter Transfer},
 journal = {Computer Applications and Software},
 volume = {37},
 number = {81-87},
 year = {2020},
 issn = {1000-386X},
 doi ={}
 }
@inproceedings{4-6,
  title={Hot-Start Transfer Learning Combined with Approximate Distillation for Mongolian-Chinese Neural Machine Translation},
  author={Wang, Pengcong and Hou, Hongxu and Sun, Shuo and Wu, Nier and Jian, Weichen and Yang, Zongheng and Wang, Yisong},
  booktitle={Machine Translation: 18th China Conference, CCMT 2022, Lhasa, China, August 6--10, 2022, Revised Selected Papers},
  pages={12--23},
  year={2022},
  organization={Springer}
}

@inproceedings{4-6b1,
author = {Zhi, Xiu and Wang, Siriguleng},
title = {Research on the Application of BERT in Mongolian-Chinese Neural Machine Translation},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457744},
doi = {10.1145/3457682.3457744},
booktitle = {2021 13th International Conference on Machine Learning and Computing},
pages = {404–409},
numpages = {6},
keywords = {BERT, Transformer, Mongolian-Chinese neural machine translation},
location = {Shenzhen, China},
series = {ICMLC 2021}
}




@article{4-8,
  title={Chinese-Uyghur bilingual lexicon extraction based on weak supervision},
  author={Aysa, Anwar and Ablimit, Mijit and Yilahun, Hankiz and Hamdulla, Askar},
  journal={Information},
  volume={13},
  number={4},
  pages={175},
  year={2022},
  publisher={MDPI}
}
@article{4-8b1,
  title={Pre-Training on Mixed Data for Low-Resource Neural Machine Translation},
  author={Zhang, Wenbo and Li, Xiao and Yang, Yating and Dong, Rui},
  journal={Information},
  volume={12},
  number={3},
  pages={133},
  year={2021},
  publisher={MDPI}
}
@inproceedings{4-8b3,
  title={Optimization of dimension-han machine translation model based on neural network},
  author={Liu, Jiatong and Wang, Zhonghao and Cui, Yanchang and Fan, Minghong and Wang, Beizhan},
  booktitle={Journal of Physics: Conference Series},
  volume={1646},
  number={1},
  pages={012143},
  year={2020},
  organization={IOP Publishing}
}
}
@article{4-9,
 author = {Hasan,Wumaier and Sirajahmat,Ruzmamat and Xireaili,Hairela and LIU,Wenqi and Tuergen,Yibulayin and WANG,Liejun and Wayit,Abulizi},
 title = {Bi-directional Uyghur-Chinese Neural Machine Translation with Marked Syllables},
 journal = {Computer Engineering and Applications},
 volume = {57},
 number = {161-168},
 year = {2021},
 issn = {1002-8331},
 doi ={}
 }
@article{4-10,
 author = {Ayiguli,Halike and Kahaerjiang,Abiderexiti and Aishan,Wumaier and Tuergen,Yibulayin},
 title = {Neural machine translation of Uyghur-Chinese quantifier},
 journal = {Computer Engineering and Design},
 volume = {40},
 number = {2649-2653},
 year = {2019},
 issn = {1000-7024},
 doi ={10.16208/j.issn1000-7024.2019.09.043}
 }
 @inproceedings{4add-one,
  title={Morphological Analysis Corpus Construction of Uyghur},
  author={Abudouwaili, Gulinigeer and Abiderexiti, Kahaerjiang and Wushouer, Jiamila and Shen, Yunfei and Maimaitimin, Turenisha and Yibulayin, Tuergen},
  booktitle={Chinese Computational Linguistics: 20th China National Conference, CCL 2021, Hohhot, China, August 13--15, 2021, Proceedings},
  pages={280--293},
  year={2021},
  organization={Springer}
}
@article{4-11,
 author = {Zhun,Shunle},
 title = {Optimized Chinese-Uyghur neural machine translation model based on multi-features},
 journal = {Computer Engineering and Design},
 volume = {40},
 number = {1484-1488},
 year = {2019},
 issn = {1000-7024},
 doi ={10.16208/j.issn1000-7024.2019.05.051}
 }
@article{4-12,
  title={Research on Uyghur-Chinese neural machine translation based on the transformer at multistrategy segmentation granularity},
  author={Xu, Zhiwang and Qin, Huibin and Hua, Yongzhu},
  journal={Mobile Information Systems},
  volume={2021},
  pages={1--7},
  year={2021},
  publisher={Hindawi Limited}
}

@article{4-13,
  title={Improving neural machine translation with sentence alignment learning},
  author={Shi, Xuewen and Huang, Heyan and Jian, Ping and Tang, Yi-Kun},
  journal={Neurocomputing},
  volume={420},
  pages={15--26},
  year={2021},
  publisher={Elsevier}
}
}

@inproceedings{shi-etal-2023-ji,
  title = "Improving Word-level Diversity in Neural Machine Translation by Controlling the Effects of Word Frequency",
  author = "Shi, Xuewen and Jian, Ping and Tang, Yikun and Huang, Heyan",
  editor = "Sun, Maosong and Qin, Bing and Qiu, Xipeng and Jiang, Jing and Han, Xianpei",
  booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
  month = aug,
  year = "2023",
  address = "Harbin, China",
  publisher = "Chinese Information Processing Society of China",
  url = "https://aclanthology.org/2023.ccl-1.6",
  pages = "64--77",
  language = "Chinese",
}


@InProceedings{YangMuyun,
  author="Yang, Muyun
  and Hu, Xixin
  and Xiong, Hao
  and Wang, Jiayi
  and Jiaermuhamaiti, Yiliyaer
  and He, Zhongjun
  and Luo, Weihua
  and Huang, Shujian",
  editor="Huang, Shujian and Knight, Kevin",
  title="CCMT 2019 Machine Translation Evaluation Report",
  booktitle="Machine Translation",
  year="2019",
  publisher="Springer Singapore",
  address="Singapore",
  pages="105--128",
  isbn="978-981-15-1721-1",
}



@inproceedings{4-15,
  title={Life Is Short, Train It Less: Neural Machine Tibetan-Chinese Translation Based on mRASP and Dataset Enhancement},
  author={Wang, Hao and Yu, Yongbin and Tashi, Nyima and Dongrub, Rinchen and Favour, Ekong and Ai, Mengwei and Gyatso, Kalzang and Cuo, Yong and Nuo, Qun},
  booktitle={Machine Translation: 18th China Conference, CCMT 2022, Lhasa, China, August 6--10, 2022, Revised Selected Papers},
  pages={54--59},
  year={2022},
  organization={Springer}
}


@article{DBLP:journals/talip/ShiWSH22,
  author       = {Shumin Shi and
                  Xing Wu and
                  Rihai Su and
                  Heyan Huang},
  title        = {Low-resource Neural Machine Translation: Methods and Trends},
  journal      = {{ACM} Trans. Asian Low Resour. Lang. Inf. Process.},
  volume       = {21},
  number       = {5},
  pages        = {103:1--103:22},
  year         = {2022},
  url          = {https://doi.org/10.1145/3524300},
  doi          = {10.1145/3524300},
  timestamp    = {Sat, 29 Apr 2023 19:27:41 +0200},
  biburl       = {https://dblp.org/rec/journals/talip/ShiWSH22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{4-16,
 author = {Cizhen,Jiacuo and Sangjie,Duanzhu and Sun,Maosong and Zhou,Maoxian and Se,Chajia},
 title = {Research on Tibetan-Chinese Machine Translation Method with Iterative Back Translation Strategy},
 journal = {Journal of Chinese Information Processing},
 volume = {34},
 number = {67-73},
 year = {2020},
 issn = {1003-0077},
 doi ={}
 }
@article{4-16b1,
 author = {Yang,Dan and Syn,Yidong and Yong,Cuo},
 title = {Research on Tibetan-Chinese Neural Machine Translation Based on Data Enhancement},
 journal = {Computer \& Digital Engineering},
 volume = {50},
 number = {2473-2477},
 year = {2022},
 issn = {1672-9722},
 doi ={}
 }

@inproceedings{4-16b2,
  title={Investigating Back-Translation in Tibetan-Chinese Neural Machine Translation},
  author={Liu, Ding and Li, Yachao and Zhu, Dengyun and Liu, Xuan and Ma, Ning and Zhu, Ao},
  booktitle={Journal of Physics: Conference Series},
  volume={1651},
  number={1},
  pages={012122},
  year={2020},
  organization={IOP Publishing}
}

@article{4-16b3,
 author = {Sun,Yidong and Yong,Cuo and Yang,Dan},
 title = {Tibetan-Chinese Bidirectional Machine Translation Based on VOLT},
 journal = {Computer and Modernization},
 volume = {No.321},
 number = {28-32},
 year = {2022},
 issn = {1006-2475},
 doi ={}
 }


@article{4-14,
 author = {Li,Yachao and Xiong,Deyi and Zang,Min and Jiang,Jing and Ma,Ning and Yin,Jianmin},
 title = {Research on Tibetan-Chinese Neural Machine Translation},
 journal = {Journal of Chinese Information Processing},
 volume = {31},
 number = {103-109},
 year = {2017},
 issn = {1003-0077},
 doi ={}
 }
 @article{4add-tow,
  title={Optimisation of the largest annotated Tibetan corpus combining rule-based, memory-based, and deep-learning methods},
  author={Meelen, Marieke and Roux, {\'E}lie and Hill, Nathan},
  journal={ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},
  volume={20},
  number={1},
  pages={1--11},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@inproceedings{4-17,
  title={Tibetan-Chinese neural machine translation combining attention mechanism},
  author={Jiang, Tao and Sun, Hao and Dai, Yu Gang and Liu, Ding},
  booktitle={Journal of Physics: Conference Series},
  volume={1607},
  number={1},
  pages={012001},
  year={2020},
  organization={IOP Publishing}
}
@inproceedings{4-18,
  title={Tibetan-chinese neural machine translation based on syllable segmentation},
  author={Lai, Wen and Zhao, Xiaobing and Bao, Wei},
  booktitle={Proceedings of the AMTA 2018 Workshop on Technologies for MT of Low Resource Languages (LoResMT 2018)},
  pages={21--29},
  year={2018}
}
@inproceedings{4-18b1,
  title={Research on Tibetan-Chinese Neural Machine Translation Integrating Syntactic Information},
  author={Zhou, Maoxian and Secha, Jia and Cai, Rangjia},
  booktitle={2021 3rd International Conference on Advanced Information Science and System (AISS 2021)},
  pages={1--4},
  year={2021}
}
@inproceedings{4-18b2,
  title={An End-to-End Method for Data Filtering on Tibetan-Chinese Parallel Corpus via Negative Sampling},
  author={Duanzhu, Sangjie and Jiacuo, Cizhen and Te, Rou and Jia, Sanzhi and Jia, Cairang},
  booktitle={Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18--20, 2019, Proceedings},
  pages={414--423},
  year={2019},
  organization={Springer}
}

@inproceedings{gyatso2023ccmt2023,
  title={CCMT2023 Tibetan-Chinese Machine Translation Evaluation Technical Report},
  author={Gyatso, Kalzang and Liu, Peizhuo and Jing, Yi and Li, Yinqiao and Tashi, Nyima and Xiao, Tong and Zhu, Jingbo},
  booktitle={China Conference on Machine Translation},
  pages={28--36},
  year={2023},
  organization={Springer}
}



@article{4-2-3,
  title={Neural machine translation of logographic languages using sub-character level information},
  author={Zhang, Longtu and Komachi, Mamoru},
  journal={arXiv preprint arXiv:1809.02694},
  year={2018}
}
@article{4-2-4,
  title={Chinese-Japanese unsupervised neural machine translation using sub-character level information},
  author={Zhang, Longtu and Komachi, Mamoru},
  journal={arXiv preprint arXiv:1903.00149},
  year={2019}
}
@article{4-2-5,
  title={Pinyin as subword unit for chinese-sourced neural machine translation},
  author={Du, Jinhua and Way, Andy},
  year={2017},
  publisher={CEUR-WS}
}
@article{4-2-6,
  title={wubi2en: Character-level chinese-english translation through ascii encoding},
  author={Tan, Mi Xue and Hu, Yuhuang and Nikolov, Nikola I and Hahnloser, Richard HR},
  journal={arXiv preprint arXiv:1805.03330},
  year={2018}
}
@INPROCEEDINGS{4-2-7-1,
  author={Zhang, Jinyi and Matsumoto, Tadahiro},
  booktitle={2019 International Conference on Asian Language Processing (IALP)}, 
  title={Character Decomposition for Japanese-Chinese Character-Level Neural Machine Translation}, 
  year={2019},
  volume={},
  number={},
  pages={35-40},
  doi={10.1109/IALP48816.2019.9037677}}

@inproceedings{4-2-8,
  title={Very large-scale lexical resources to enhance Chinese and Japanese machine translation},
  author={Halpern, Jack},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}
@article{4-2-9,
  title={Inference-only sub-character decomposition improves translation of unseen logographic characters},
  author={Saunders, Danielle and Feely, Weston and Byrne, Bill},
  journal={arXiv preprint arXiv:2011.06523},
  year={2020}
}
@INPROCEEDINGS{4-2-9-1,
  author={Zhang, Jinyi and Matsumoto, Tadahiro},
  booktitle={2017 International Conference on Asian Language Processing (IALP)}, 
  title={Japanese-chinese machine translation for the Japanese case particle "de"}, 
  year={2017},
  volume={},
  number={},
  pages={330-333},
  doi={10.1109/IALP.2017.8300610}
}
@INPROCEEDINGS{4-2-9-2,
  author={Zhang, Jinyi and Matsumoto, Tadahiro},
  booktitle={2017 International Conference on Asian Language Processing (IALP)}, 
  title={Improving character-level Japanese-Chinese neural machine translation with radicals as an additional input feature}, 
  year={2017},
  volume={},
  number={},
  pages={172-175},
  doi={10.1109/IALP.2017.8300572}
}

@article{4-2-10,
  title={Improvement of Japanese Chinese machine translation system},
  author={Nakazawa Toshiaki and Kurohashi Yoshio},
  journal={Japio year book},
  pages={258--261},
  year={2012},
  publisher={Japio year book},
  url={https://www.japio.or.jp/00yearbook/files/2012book/12_4_05.pdf}
}
@phdthesis{4-2-11,
  title={A Study on Japanese-Chinese Machine Translation -- Centering on the Rules for TORITATE Expression and Negative Expression},
  author={Bu, Zhao Hui},
  year={2004},
  school={Gifu University}
}
@article{4-2-12,
  title={Analyzing the problems of vocabulary in Japanese-Chinese neural network machine translation},
  author={Luo, Wentao},
  journal={Computer Science and Application},
  volume={10},
  number={3},
  pages={387--397},
  year={2020}
}
@article{4-2-13,
 author = {Zhao,ZhiYun and Shi,ChongDe and He,YanQing and Gao,YingFan and Yao,ChangQing},
 title = {Cooperative Research on Chinese-Japanese Machine Translation for S\&T Documents},
 journal = {Technology Intelligence Engineering},
 volume = {3},
 number = {4-9},
 year = {2017},
 issn = {2095-915X},
 doi ={}
 }
@article{4-2-14,
 author = {Li Yipeng},
 title = {Japanese Technological Language Labeling Techniques in the Sino-Japanese Bilingual Parallel Corpus},
 journal = {Guide to Business},
 volume = {No.282},
 number = {175-176},
 year = {2015},
 issn = {1671-1599},
 doi ={10.19354/j.cnki.42-1616/f.2015.02.110}
 }
@article{4-2-18,
  title={Is word segmentation necessary for deep learning of Chinese representations?},
  author={Li, Xiaoya and Meng, Yuxian and Sun, Xiaofei and Han, Qinghong and Yuan, Arianna and Li, Jiwei},
  journal={arXiv preprint arXiv:1905.05526},
  year={2019}
}
@inproceedings{4one,
  title={Constructing a Chinese―Japanese Parallel Corpus from Wikipedia.},
  author={Chu, Chenhui and Nakazawa, Toshiaki and Kurohashi, Sadao},
  booktitle={LREC},
  pages={642--647},
  year={2014}
}
@article{4tow,
  title={Corpus augmentation for neural machine translation with Chinese-Japanese parallel corpora},
  author={Zhang, Jinyi and Matsumoto, Tadahiro},
  journal={Applied sciences},
  volume={9},
  number={10},
  pages={2036},
  year={2019},
  publisher={MDPI}
}
@article{4three,
  title={Parallel corpus filtering via pre-trained language models},
  author={Zhang, Boliang and Nagesh, Ajay and Knight, Kevin},
  journal={arXiv preprint arXiv:2005.06166},
  year={2020}
}
@article{4four,
  title={WCC-JC: a web-crawled corpus for Japanese-Chinese neural machine translation},
  author={Zhang, Jinyi and Tian, Ye and Mao, Jiannan and Han, Mei and Matsumoto, Tadahiro},
  journal={Applied Sciences},
  volume={12},
  number={12},
  pages={6002},
  year={2022},
  publisher={MDPI}
}
@article{4five,
  title={WCC-JC 2.0: A Web-Crawled and Manually Aligned Parallel Corpus for Japanese-Chinese Neural Machine Translation},
  author={Zhang, Jinyi and Tian, Ye and Mao, Jiannan and Han, Mei and Wen, Feng and Guo, Cong and Gao, Zhonghui and Matsumoto, Tadahiro},
  journal={Electronics},
  volume={12},
  number={5},
  pages={1140},
  year={2023},
  publisher={MDPI}
}
@article{4-2-19,
  title={Chinese-English-Burmese neural machine translation based on multilingual joint training},
  author={Man,Zhibo and Mao,Cunli and Yu,Zhengtao and Li,Xunyu and Gao,Shengxiang and Zhu,Junguo},
  journal={Journal of Tsinghua University (Science and Technology)},
  volume={61},
  number={9},
  pages={927--935},
  year={2021}
}
@mastersthesis{4-2-20,
 author = {Zhang Shaoning},
 title = {Research on the Construction Method of Chinese-Myanmar Parallel Corpus Based on Pivot Language},
 school = {Kunming University of Science and Technology},
 year = {2019},
 URL={}
 }
@article{4-2-21,
 author = {Li,Xunyu and Mao, Cunli and Yu,Zhengtao and Gao, Shengxiang and Wang, Zhenhan and Zhang, Yafei},
 title = {Chinese-Burmese Comparable Document Acquisition Based on Topic Model and Bilingual Word Embedding},
 journal = {Journal of Chinese Information Processing},
 volume = {35},
 number = {88-95},
 year = {2021},
 issn = {1003-0077},
 doi ={}
 }
@mastersthesis{4-2-22,
 author = {Li,Yue},
 title = {Research on Chinese-Myanmar Neural Machine Translation Method with Single Language Material},
 school = {Kunming University of Science and Technology},
 year = {2020},
 }
@article{4-2-23,
  title={Chinese-Myanmar parallel sentence pairs generation method based on semantic difference},
  author={Yu,Zhiqiang and Wen,Yonghua and Gao,Minghu and Yang,Man},
  journal = {ournal of Yunnan University of Nationalities: Natural Sciences Edition},
  year = {2023},
  volume = {32},
  number={1},
  pages={118-123}
}
@article{4-2-24,
author = {Li,Yue and Mao,Cunli and Yu,Zhengtao and Gao,Shengxiang and Wang,Zhenhan and Zhang,Yafei},
title = {Method of Chinese Burmese Bilingual Vocabulary Extraction Based on Subject and Context Features},
publisher = {Journal of Chinese Computer Systems},
year = {2021},
journal = {Journal of Chinese Computer Systems},
volume = {42},
number = {1},
eid = {91},
numpages = {4},
pages = {91},
keywords = {Chinese-Myanmar vocabulary;thematic features;contextual features;BERT;bilingual word vector},
}    
@article{corpus,
  title={Towards Burmese (Myanmar) morphological analysis: Syllable-based tokenization and part-of-speech tagging},
  author={Ding, Chenchen and Aye, Hnin Thu Zar and Pa, Win Pa and Nwet, Khin Thandar and Soe, Khin Mar and Utiyama, Masao and Sumita, Eiichiro},
  journal={ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},
  volume={19},
  number={1},
  pages={1--34},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{4-2-25,
 author = {Mao,Cunli and Wu,Xia and Zhu,Junguo and Yu,Zhengtao and Li,Yunlong and Wang,Zhenhan},
 title = {Chinese-Burmese Parallel Sentence Pair Extraction Based on CNN-CorrNet},
 journal = {Journal of Chinese Information Processing},
 volume = {34},
 number = {60-66},
 year = {2020},
 issn = {1003-0077},
 doi ={}
 }
@article{4-2-26,
  title={Semi-supervised Chinese-Burmese Bilingual Dictionary Construction},
  author={ Mao,Cunli and Lu,Shan and Wang,Hongbin and Yu,Zhengtao and Wu,Xia and Wang,Zhenhan},
  journal={Journal of Chinese Information Processing},
  volume={35},
  number={7},
  pages={47--53},
  year={2021}
}
@inproceedings{4-2-27,
  title={Semi-Supervised Chinese-Myanmar Neural Machine Translation based Model-Uncertainty},
  author={Wang, Linqin and Yu, Zhengtao and Mao, Cunli and Gao, Chengxiang and Man, Zhibo and Wang, Zhenhan},
  booktitle={Proceedings of the 20th Chinese National Conference on Computational Linguistics},
  pages={35--45},
  year={2021}
}





@article{johnson2017google,
  title={Google’s multilingual neural machine translation system: Enabling zero-shot translation},
  author={Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={5},
  pages={339--351},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{dong2015multi,
  title={Multi-task learning for multiple language translation},
  author={Dong, Daxiang and Wu, Hua and He, Wei and Yu, Dianhai and Wang, Haifeng},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1723--1732},
  year={2015}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{kim2019pivot,
  title={Pivot-based transfer learning for neural machine translation between non-English languages},
  author={Kim, Yunsu and Petrov, Petre and Petrushkov, Pavel and Khadivi, Shahram and Ney, Hermann},
  journal={arXiv preprint arXiv:1909.09524},
  year={2019}
}



@article{4-2-28,
 author = {Zhang,Hongtao and Wen,Yonghua and Wang,Jian},
 title = {Thai-Chinese Neural Machine Translation Method Based on Dependency Distance Penalty},
 journal = {Communications Technology},
 volume = {55},
 number = {990-997},
 year = {2022},
 issn = {1002-0802},
 doi ={}
 }
@mastersthesis{4-2-29,
 author = {Zhang,Yihan},
 title = {Research on Thai-Chinese Machine Translation Optimization Method under Low Resource Conditions},
 school = {Yunnan University},
 year = {2021}
 }
@article{4-2-30,
  title={Neural machine translation integrating bidirectional-dependency self-attention mechanism},
  author={Li,Zhijin and Lai,Hua and Wen,Yonghua and Gao,Shengxiang},
  journal={Journal of Computer Applications},
  volume={42},
  number={12},
  pages={3679},
  year={2022}
}
@mastersthesis{4-2-31,
 author = {Feng Yinhan},
 title = {A Study on the Method of Computing Sentence Similarity between Chinese and Thai Languages Based on Word Embedding},
 school = {Kunming University of Science and Technology},
 year = {2019},
 URL={}
 }
@article{thai-corpus,
  title={A large english--thai parallel corpus from the web and machine-generated text},
  author={Lowphansirikul, Lalita and Polpanumas, Charin and Rutherford, Attapol T and Nutanong, Sarana},
  journal={Language Resources and Evaluation},
  volume={56},
  number={2},
  pages={477--499},
  year={2022},
  publisher={Springer}
}
@article{4-2-32,
 author = {Liu,Yan and Xiong,Deyi},
 title = {Construction Method of Parallel Corpus for Minority Language Machine Translation},
 journal = {Computer Science},
 volume = {49},
 number = {41-46},
 year = {2022},
 issn = {1002-137X},
 doi ={}
 }
@inproceedings{4-2-33,
  title={Transfer Learning for Chinese-Lao Neural Machine Translation with Linguistic Similarity},
  author={Yu, Zhiqiang and Yu, Zhengtao and Huang, Yuxin and Guo, Junjun and Wang, Zhenhan and Man, Zhibo},
  booktitle={Machine Translation: 16th China Conference, CCMT 2020, Hohhot, China, October 10-12, 2020, Revised Selected Papers 16},
  pages={1--10},
  year={2020},
  organization={Springer}
}
@article{4-2-34,
  title={TURJUMAN: A Public Toolkit for Neural Arabic Machine Translation},
  author={Nagoudi, El Moatez Billah and Elmadany, AbdelRahim and Abdul-Mageed, Muhammad},
  journal={arXiv preprint arXiv:2206.03933},
  year={2022}
}
@article{4-2-36,
  title={ Domain adaptation approach for low resource Russian-Chinese machine translation task},
  author={ Liu,Huan and Liu,Junpeng and Huang,Kaiyu and Huang,Degen},
  journal={Journal of Xiamen University(Natural Science)},
  year={2022}
}
@article{4-2-37,
  title={ Neural machine translation corpus expansion method based on language similarity mining},
  author={Can,LI and Yating,YANG and Yupeng,MA and Rui,DONG},
  journal={Journal of Computer Applictions},
  volume={41},
  number={11},
  pages={3145},
  year={2021}
}
%**************************************************************************************
@inproceedings{6-1,
  title={The AISP-SJTU Translation System for WMT 2022},
  author={Liu, Guangfeng and Zhu, Qinpei and Chen, Xingyu and Feng, Renjie and Ren, Jianxin and Wu, Renshou and Miao, Qingliang and Wang, Rui and Yu, Kai},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={310--317},
  year={2022}
}
@inproceedings{6-2,
  title={Inria-ALMAnaCH at WMT 2022: Does Transcription Help Cross-Script Machine Translation?},
  author={Alabi, Jesujoba and Nishimwe, Lydia and Muller, Benjamin and Rey, Camille and Sagot, Beno{\^\i}t and Bawden, Rachel},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={233--243},
  year={2022}
}
@article{6-3,
  title={Adam Mickiewicz University at WMT 2022: NER-Assisted and Quality-Aware Neural Machine Translation},
  author={Nowakowski, Artur and Pa{\l}ka, Gabriela and Guttmann, Kamil and Pokrywka, Miko{\l}aj},
  journal={arXiv preprint arXiv:2209.02962},
  year={2022}
}
@inproceedings{6-4,
  title={The ARC-NKUA submission for the English-Ukrainian General Machine Translation Shared Task at WMT22},
  author={Roussis, Dimitrios and Papavassiliou, Vassilis},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={358--365},
  year={2022}
}
@inproceedings{6-4-1,
  title={In neural machine translation, what does transfer learning transfer?},
  author={Aji, Alham Fikri and Bogoychev, Nikolay and Heafield, Kenneth and Sennrich, Rico},
  year={2020},
  organization={Association for Computational Linguistics}
}
@article{6-4-2,
  title={Transfer learning for low-resource neural machine translation},
  author={Zoph, Barret and Yuret, Deniz and May, Jonathan and Knight, Kevin},
  journal={arXiv preprint arXiv:1604.02201},
  year={2016}
}
@article{6-4-3,
  title={Understanding back-translation at scale},
  author={Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
  journal={arXiv preprint arXiv:1808.09381},
  year={2018}
}
@inproceedings{6-4-4,
  title={Incorporating external annotation to improve named entity translation in NMT},
  author={Modrzejewski, Maciej and Exel, Miriam and Buschbeck, Bianka and Ha, Thanh-Le and Waibel, Alex},
  booktitle={Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
  pages={45--51},
  year={2020}
}
@article{6-4-5,
  title={Quality-aware decoding for neural machine translation},
  author={Fernandes, Patrick and Farinhas, Ant{\'o}nio and Rei, Ricardo and de Souza, Jos{\'e} GC and Ogayo, Perez and Neubig, Graham and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2205.00978},
  year={2022}
}
@inproceedings{6-4-6,
  title={Multi-domain neural machine translation through unsupervised adaptation},
  author={Farajian, M Amin and Turchi, Marco and Negri, Matteo and Federico, Marcello},
  booktitle={Proceedings of the Second Conference on Machine Translation},
  pages={127--137},
  year={2017}
}
@inproceedings{6-5,
  title={CUNI-Bergamot Submission at WMT22 General Translation Task},
  author={Jon, Josef and Popel, Martin and Bojar, Ond{\v{r}}ej},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={280--289},
  year={2022}
}

@article{6-6,
  title={CUNI Systems for the WMT22 Czech-Ukrainian Translation Task},
  author={Popel, Martin and Libovick{\`y}, Jind{\v{r}}ich and Helcl, Jind{\v{r}}ich},
  journal={arXiv preprint arXiv:2212.00486},
  year={2022}
}
@article{6-6-1,
  title={Marian: Fast neural machine translation in C++},
  author={Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and Dwojak, Tomasz and Hoang, Hieu and Heafield, Kenneth and Neckermann, Tom and Seide, Frank and Germann, Ulrich and Aji, Alham Fikri and Bogoychev, Nikolay and others},
  journal={arXiv preprint arXiv:1804.00344},
  year={2018}
}
@article{6-6-2,
  title={Tensor2tensor for neural machine translation},
  author={Vaswani, Ashish and Bengio, Samy and Brevdo, Eugene and Chollet, Francois and Gomez, Aidan N and Gouws, Stephan and Jones, Llion and Kaiser, {\L}ukasz and Kalchbrenner, Nal and Parmar, Niki and others},
  journal={arXiv preprint arXiv:1803.07416},
  year={2018}
}







@inproceedings{6-7,
  title={Gtcom neural machine translation systems for wmt22},
  author={Zong, Hao and Bei, Chao},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={428--431},
  year={2022}
}
@inproceedings{6-8,
  title={Hw-tsc’s submissions to the wmt 2022 general machine translation shared task},
  author={Wei, Daimeng and Rao, Zhiqiang and Wu, Zhanglin and Li, Shaojun and Luo, Yuanchang and Xie, Yuhao and Chen, Xiaoyu and Shang, Hengchao and Li, Zongyao and Yu, Zhengzhe and others},
  booktitle={Proceedings of the Seventh Conference on Machine Translation, Online. Association for Computational Linguistics},
  year={2022}
}
@article{6-9,
  title={Vega-mt: The jd explore academy translation system for wmt22},
  author={Zan, Changtong and Peng, Keqin and Ding, Liang and Qiu, Baopu and Liu, Boan and He, Shwai and Lu, Qingyu and Zhang, Zheng and Liu, Chuang and Liu, Weifeng and others},
  journal={arXiv preprint arXiv:2209.09444},
  year={2022}
}
@inproceedings{6-10,
  title={Kyb general machine translation systems for wmt22},
  author={Kalkar, Shivam and Matsuzaki, Yoko and Li, Ben},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={290--294},
  year={2022}
}
@inproceedings{6-11,
  title={Evaluating Corpus Cleanup Methods in the WMT’22 News Translation Task},
  author={Malli, Marilena and Tambouratzis, George},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={335--341},
  year={2022}
}
@inproceedings{6-12,
  title={Lan-bridge mt’s participation in the wmt 2022 general translation shared task},
  author={Han, Bing and Wu, Yangjian and Hu, Gang and Chen, Qiulin},
  booktitle={Proceedings of the Seventh Conference on Machine Translation, Online. Association for Computational Linguistics},
  year={2022}
}

@inproceedings{6-13,
  title={No domain left behind},
  author={Zeng, Hui},
  booktitle={Proceedings of the Seventh Conference on Machine Translation, Online. Association for Computational Linguistics},
  year={2022}
}
@inproceedings{6-14,
  title={Machine translation for Livonian: Catering to 20 speakers},
  author={Rikters, Mat{\=\i}ss and Tomingas, Marili and Tuisk, Tuuli and Ern{\v{s}}treits, Valts and Fishel, Mark},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={508--514},
  year={2022}
}
@inproceedings{6-15,
  title={NAIST-NICT-TIT WMT22 General MT Task Submission},
  author={Deguchi, Hiroyuki and Imamura, Kenji and Kaneko, Masahiro and Nishida, Yuto and Sakai, Yusuke and Vasselli, Justin and Vu, Huy-Hien and Watanabe, Taro},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={244--250},
  year={2022}
}
@inproceedings{6-16,
  title={NT5 at WMT 2022 General Translation Task},
  author={Morishita, Makoto and Kudo, Keito and Oka, Yui and Chousa, Katsuki and Kiyono, Shun and Takase, Sho and Suzuki, Jun},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={318--325},
  year={2022}
}
@inproceedings{6-17,
  title={The NiuTrans Machine Translation Systems for WMT22},
  author={Shan, Weiqiao and Cao, Zhiquan and Han, Yuchen and Wu, Siming and Hu, Yimin and Wang, Jie and Zhang, Yi and Baoyu, Hou and Cao, Hang and Gao, Chenghao and others},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={366--374},
  year={2022}
}
@inproceedings{6-18,
  title={PROMT Systems for WMT22 General Translation Task},
  author={Molchanov, Alexander and Kovalenko, Vladislav and Makhamalkina, Natalia},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={342--345},
  year={2022}
}
@inproceedings{6-19,
  title={Samsung R\&D Institute Poland Participation in WMT 2022},
  author={Dobrowolski, Adam and Klimaszewski, Mateusz and My{\'s}liwy, Adam and Szyma{\'n}ski, Marcin and Kowalski, Jakub and Szypu{\l}a, Kornelia and Przew{\l}ocki, Pawe{\l} and Przybysz, Pawe{\l}},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={251--259},
  year={2022}
}
@article{6-20,
  title={Tencent ai lab-shanghai jiao tong university low-resource translation system for the wmt22 translation task},
  author={He, Zhiwei and Wang, Xing and Tu, Zhaopeng and Shi, Shuming and Wang, Rui},
  journal={arXiv preprint arXiv:2210.08742},
  year={2022}
}
@inproceedings{6-21,
  title={Teaching unseen low-resource languages to large translation models},
  author={Tars, Maali and Purason, Taido and T{\"a}ttar, Andre},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={375--380},
  year={2022}
}
@inproceedings{6-22,
  title={eTranslation’s Submissions to the WMT22 General Machine Translation Task},
  author={Oravecz, Csaba and Bontcheva, Katina and Kolovratn{\'\i}k, David and Kovachev, Bogomil and Scott, Christopher},
  booktitle={Proceedings of the Seventh Conference on Machine Translation (WMT)},
  pages={346--351},
  year={2022}
}
@inproceedings{6-23,
  title={Manifold’s english-chinese system at wmt22 general mt task},
  author={Jin, Chang and Shi, Tingxun and Xue, Zhengshan and Lin, Xiaodong},
  booktitle={Proceedings of the Seventh Conference on Machine Translation, Online. Association for Computational Linguistics},
  year={2022}
}


@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

%******************************************************************************************



@inproceedings{MaoDLSCK23,
  author       = {Zhuoyuan Mao and
                  Raj Dabre and
                  Qianying Liu and
                  Haiyue Song and
                  Chenhui Chu and
                  Sadao Kurohashi},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Exploring the Impact of Layer Normalization for Zero-shot Neural Machine
                  Translation},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 2: Short Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {1300--1316},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://aclanthology.org/2023.acl-short.112},
  timestamp    = {Thu, 13 Jul 2023 16:47:40 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/MaoDLSCK23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{aharoni-etal-2019-massively,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}

@inproceedings{al-shedivat-parikh-2019-consistency,
    title = "Consistency by Agreement in Zero-Shot Neural Machine Translation",
    author = "Al-Shedivat, Maruan  and
      Parikh, Ankur",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1121",
    doi = "10.18653/v1/N19-1121",
    pages = "1184--1197",
    abstract = "Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest. In this paper, we focus on zero-shot generalization{---}a challenging setup that tests models on translation directions they have not been optimized for at training time. To solve the problem, we (i) reformulate multilingual translation as probabilistic inference, (ii) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks, and (iii) introduce a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages. We test our multilingual NMT models on multiple public zero-shot translation benchmarks (IWSLT17, UN corpus, Europarl) and show that agreement-based learning often results in 2-3 BLEU zero-shot improvement over strong baselines without any loss in performance on supervised translation directions.",
}

@article{abs-1903-07091,
  author       = {Naveen Arivazhagan and
                  Ankur Bapna and
                  Orhan Firat and
                  Roee Aharoni and
                  Melvin Johnson and
                  Wolfgang Macherey},
  title        = {The Missing Ingredient in Zero-Shot Neural Machine Translation},
  journal      = {CoRR},
  volume       = {abs/1903.07091},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.07091},
  eprinttype    = {arXiv},
  eprint       = {1903.07091},
  timestamp    = {Mon, 01 Apr 2019 14:07:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-07091.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{chen-etal-2021-zero,
    title = "Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders",
    author = "Chen, Guanhua  and
      Ma, Shuming  and
      Chen, Yun  and
      Dong, Li  and
      Zhang, Dongdong  and
      Pan, Jia  and
      Wang, Wenping  and
      Wei, Furu",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.2",
    doi = "10.18653/v1/2021.emnlp-main.2",
    pages = "15--26",
    abstract = "Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder (MPE), or improving the performance on supervised machine translation with BERT. However, it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model. In this paper, we focus on a zero-shot cross-lingual transfer task in NMT. In this task, the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE, then it is directly tested on zero-shot language pairs. We propose SixT, a simple yet effective model for this task. SixT leverages the MPE with a two-stage training schedule and gets further improvement with a position disentangled encoder and a capacity-enhanced decoder. Using this method, SixT significantly outperforms mBART, a pretrained multilingual encoder-decoder model explicitly designed for NMT, with an average improvement of 7.1 BLEU on zero-shot any-to-English test sets across 14 source languages. Furthermore, with much less training computation cost and training data, our model achieves better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.",
}

@inproceedings{ijcai2017p555,
  author    = {Yong Cheng and Qian Yang and Yang Liu and Maosong Sun and Wei Xu},
  title     = {Joint Training for Pivot-based Neural Machine Translation},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {3974--3980},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/555},
  url       = {https://doi.org/10.24963/ijcai.2017/555},
}

@article{JMLR:v22:20-1307,
  author  = {Angela Fan and Shruti Bhosale and Holger Schwenk and Zhiyi Ma and Ahmed El-Kishky and Siddharth Goyal and Mandeep Baines and Onur Celebi and Guillaume Wenzek and Vishrav Chaudhary and Naman Goyal and Tom Birch and Vitaliy Liptchinsky and Sergey Edunov and Michael Auli and Armand Joulin},
  title   = {Beyond English-Centric Multilingual Machine Translation},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {107},
  pages   = {1--48},
  url     = {http://jmlr.org/papers/v22/20-1307.html}
}

@inproceedings{firat-etal-2016-multi,
    title = "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",
    author = "Firat, Orhan  and
      Cho, Kyunghyun  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1101",
    doi = "10.18653/v1/N16-1101",
    pages = "866--875",
}

@inproceedings{gao-etal-2022-bi,
    title = "{B}i-{S}im{C}ut: A Simple Strategy for Boosting Neural Machine Translation",
    author = "Gao, Pengzhi  and
      He, Zhongjun  and
      Wu, Hua  and
      Wang, Haifeng",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.289",
    doi = "10.18653/v1/2022.naacl-main.289",
    pages = "3938--3948",
    abstract = "We introduce Bi-SimCut: a simple but effective training strategy to boost neural machine translation (NMT) performance. It consists of two procedures: bidirectional pretraining and unidirectional finetuning. Both procedures utilize SimCut, a simple regularization method that forces the consistency between the output distributions of the original and the cutoff sentence pairs. Without leveraging extra dataset via back-translation or integrating large-scale pretrained model, Bi-SimCut achieves strong translation performance across five translation benchmarks (data sizes range from 160K to 20.2M): BLEU scores of 31.16 for $\texttt{en}\rightarrow\texttt{de}$ and 38.37 for $\texttt{de}\rightarrow\texttt{en}$ on the IWSLT14 dataset, 30.78 for $\texttt{en}\rightarrow\texttt{de}$ and 35.15 for $\texttt{de}\rightarrow\texttt{en}$ on the WMT14 dataset, and 27.17 for $\texttt{zh}\rightarrow\texttt{en}$ on the WMT17 dataset. SimCut is not a new method, but a version of Cutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be considered as a perturbation-based method. Given the universality and simplicity of Bi-SimCut and SimCut, we believe they can serve as strong baselines for future NMT research.",
}

@inproceedings{gu-etal-2018-universal,
    title = "Universal Neural Machine Translation for Extremely Low Resource Languages",
    author = "Gu, Jiatao  and
      Hassan, Hany  and
      Devlin, Jacob  and
      Li, Victor O.K.",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1032",
    doi = "10.18653/v1/N18-1032",
    pages = "344--354",
    abstract = "In this paper, we propose a new universal machine translation approach focusing on languages with a limited amount of parallel data. Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language. The lexical part is shared through a Universal Lexical Representation to support multi-lingual word-level sharing. The sentence-level sharing is represented by a model of experts from all source languages that share the source encoders with all other languages. This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages. Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multi-lingual training and back-translation. Furthermore, we show that the proposed approach can achieve almost 20 BLEU on the same dataset through fine-tuning a pre-trained multi-lingual system in a zero-shot setting.",
}

@inproceedings{gu-etal-2019-improved,
    title = "Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations",
    author = "Gu, Jiatao  and
      Wang, Yong  and
      Cho, Kyunghyun  and
      Li, Victor O.K.",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1121",
    doi = "10.18653/v1/P19-1121",
    pages = "1258--1268",
    abstract = "Zero-shot translation, translating between language pairs on which a Neural Machine Translation (NMT) system has never been trained, is an emergent property when training the system in multilingual settings. However, naive training for zero-shot NMT easily fails, and is sensitive to hyper-parameter setting. The performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot. In this work, we address the degeneracy problem due to capturing spurious correlations by quantitatively analyzing the mutual information between language IDs of the source and decoded sentences. Inspired by this analysis, we propose to use two simple but effective approaches: (1) decoder pre-training; (2) back-translation. These methods show significant improvement (4 22 BLEU points) over the vanilla zero-shot translation on three challenging multilingual datasets, and achieve similar or better results than the pivot-based approach.",
}

@inproceedings{gu-feng-2022-improving,
    title = "Improving Zero-Shot Multilingual Translation with Universal Representations and Cross-Mapping",
    author = "Gu, Shuhao  and
      Feng, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.485",
    pages = "6492--6504",
    abstract = "The many-to-many multilingual neural machine translation can translate between language pairs unseen during training, i.e., zero-shot translation. Improving zero-shot translation requires the model to learn universal representations and cross-mapping relationships to transfer the knowledge learned on the supervised directions to the zero-shot directions. In this work, we propose the state mover{'}s distance based on the optimal theory to model the difference of the representations output by the encoder. Then, we bridge the gap between the semantic-equivalent representations of different languages at the token level by minimizing the proposed distance to learn universal representations. Besides, we propose an agreement-based training scheme, which can help the model make consistent predictions based on the semantic-equivalent sentences to learn universal cross-mapping relationships for all translation directions. The experimental results on diverse multilingual datasets show that our method can improve consistently compared with the baseline system and other contrast methods. The analysis proves that our method can better align the semantic space and improve the prediction consistency.",
}

@inproceedings{ha-etal-2016-toward,
    title = "Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder",
    author = "Ha, Thanh-Le  and
      Niehues, Jan  and
      Waibel, Alex",
    booktitle = "Proceedings of the 13th International Conference on Spoken Language Translation",
    month = dec # " 8-9",
    year = "2016",
    address = "Seattle, Washington D.C",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2016.iwslt-1.6",
    abstract = "In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under a unified approach in which the information shared among languages can be helpful in the translation of individual language pairs. We are then able to employ attention-based Neural Machine Translation for many-to-many multilingual translation tasks. Our approach does not require any special treatment on the network architecture and it allows us to learn minimal number of free parameters in a standard way of training. Our approach has shown its effectiveness in an under-resourced translation scenario with considerable improvements up to 2.6 BLEU points. In addition, we point out a novel way to make use of monolingual data with Neural Machine Translation using the same approach with a 3.15-BLEU-score gain in IWSLT{'}16 English→German translation task.",
}

@inproceedings{JiZDZCL20,
  author       = {Baijun Ji and
                  Zhirui Zhang and
                  Xiangyu Duan and
                  Min Zhang and
                  Boxing Chen and
                  Weihua Luo},
  title        = {Cross-Lingual Pre-Training Based Transfer for Zero-Shot Neural Machine
                  Translation},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {115--122},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/5341},
  timestamp    = {Mon, 07 Mar 2022 16:58:11 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/JiZDZCL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1162/tacl_a_00065,
    author = {Johnson, Melvin and Schuster, Mike and Le, Quoc
                            V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
    title = "{Google’s Multilingual Neural Machine Translation System: Enabling
                    Zero-Shot Translation}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {5},
    pages = {339-351},
    year = {2017},
    month = {10},
    abstract = "{We propose a simple solution to use a single Neural Machine Translation (NMT)
                    model to translate between multiple languages. Our solution requires no changes
                    to the model architecture from a standard NMT system but instead introduces an
                    artificial token at the beginning of the input sentence to specify the required
                    target language. Using a shared wordpiece vocabulary, our approach enables
                    Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a
                    single multilingual model achieves comparable performance for English→French and
                    surpasses state-of-theart results for English→German. Similarly, a single
                    multilingual model surpasses state-of-the-art results for French→English and
                    German→English on WMT’14 and WMT’15 benchmarks, respectively. On production
                    corpora, multilingual models of up to twelve language pairs allow for better
                    translation of many individual pairs. Our models can also learn to perform
                    implicit bridging between language pairs never seen explicitly during training,
                    showing that transfer learning and zero-shot translation is possible for neural
                    translation. Finally, we show analyses that hints at a universal interlingua
                    representation in our models and also show some interesting examples when mixing
                    languages.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00065},
    url = {https://doi.org/10.1162/tacl\_a\_00065},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00065/1567476/tacl\_a\_00065.pdf},
}

@inproceedings{lin-etal-2020-pre,
    title = "Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information",
    author = "Lin, Zehui  and
      Pan, Xiao  and
      Wang, Mingxuan  and
      Qiu, Xipeng  and
      Feng, Jiangtao  and
      Zhou, Hao  and
      Li, Lei",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.210",
    doi = "10.18653/v1/2020.emnlp-main.210",
    pages = "2649--2663",
    abstract = "We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at https://github. com/linzehui/mRASP.",
}

@inproceedings{liu-etal-2021-improving-zero,
    title = "Improving Zero-Shot Translation by Disentangling Positional Information",
    author = "Liu, Danni  and
      Niehues, Jan  and
      Cross, James  and
      Guzm{\'a}n, Francisco  and
      Li, Xian",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.101",
    doi = "10.18653/v1/2021.acl-long.101",
    pages = "1259--1273",
    abstract = "Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e. zero-shot translation. Despite being conceptually attractive, it often suffers from low output quality. The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training. We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens. We show that this can be easily alleviated by removing residual connections in an encoder layer. With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions. The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation. Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage. By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.",
}

@inproceedings{lu-etal-2018-neural,
    title = "A neural interlingua for multilingual machine translation",
    author = "Lu, Yichao  and
      Keung, Phillip  and
      Ladhak, Faisal  and
      Bhardwaj, Vikas  and
      Zhang, Shaonan  and
      Sun, Jason",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6309",
    doi = "10.18653/v1/W18-6309",
    pages = "84--92",
    abstract = "We incorporate an explicit neural interlingua into a multilingual encoder-decoder neural machine translation (NMT) architecture. We demonstrate that our model learns a language-independent representation by performing direct zero-shot translation (without using pivot translation), and by using the source sentence embeddings to create an English Yelp review classifier that, through the mediation of the neural interlingua, can also classify French and German reviews. Furthermore, we show that, despite using a smaller number of parameters than a pairwise collection of bilingual NMT models, our approach produces comparable BLEU scores for each language pair in WMT15.",
}

@inproceedings{pan-etal-2021-contrastive,
    title = "Contrastive Learning for Many-to-many Multilingual Neural Machine Translation",
    author = "Pan, Xiao  and
      Wang, Mingxuan  and
      Wu, Liwei  and
      Li, Lei",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.21",
    doi = "10.18653/v1/2021.acl-long.21",
    pages = "244--258",
    abstract = "Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model. mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline",
}

@inproceedings{pham-etal-2019-improving,
    title = "Improving Zero-shot Translation with Language-Independent Constraints",
    author = "Pham, Ngoc-Quan  and
      Niehues, Jan  and
      Ha, Thanh-Le  and
      Waibel, Alexander",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5202",
    doi = "10.18653/v1/W19-5202",
    pages = "13--23",
    abstract = "An important concern in training multilingual neural machine translation (NMT) is to translate between language pairs unseen during training, i.e zero-shot translation. Improving this ability kills two birds with one stone by providing an alternative to pivot translation which also allows us to better understand how the model captures information between languages. In this work, we carried out an investigation on this capability of the multilingual NMT models. First, we intentionally create an encoder architecture which is independent with respect to the source language. Such experiments shed light on the ability of NMT encoders to learn multilingual representations, in general. Based on such proof of concept, we were able to design regularization methods into the standard Transformer model, so that the whole architecture becomes more robust in zero-shot conditions. We investigated the behaviour of such models on the standard IWSLT 2017 multilingual dataset. We achieved an average improvement of 2.23 BLEU points across 12 language pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots.",
}

@inproceedings{wang-etal-2021-rethinking-zero,
    title = "Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables",
    author = "Wang, Weizhi  and
      Zhang, Zhirui  and
      Du, Yichao  and
      Chen, Boxing  and
      Xie, Jun  and
      Luo, Weihua",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.366",
    doi = "10.18653/v1/2021.findings-emnlp.366",
    pages = "4321--4327",
    abstract = "Zero-shot translation, directly translating between language pairs unseen in training, is a promising capability of multilingual neural machine translation (NMT). However, it usually suffers from capturing spurious correlations between the output language and language invariant semantics due to the maximum likelihood training objective, leading to poor transfer performance on zero-shot translation. In this paper, we introduce a denoising autoencoder objective based on pivot language into traditional training objective to improve the translation accuracy on zero-shot directions. The theoretical analysis from the perspective of latent variables shows that our approach actually implicitly maximizes the probability distributions for zero-shot directions. On two benchmark machine translation datasets, we demonstrate that the proposed method is able to effectively eliminate the spurious correlations and significantly outperforms state-of-the-art methods with a remarkable performance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.",
}

@inproceedings{wu-etal-2021-language,
    title = "Language Tags Matter for Zero-Shot Neural Machine Translation",
    author = "Wu, Liwei  and
      Cheng, Shanbo  and
      Wang, Mingxuan  and
      Li, Lei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.264",
    doi = "10.18653/v1/2021.findings-acl.264",
    pages = "3001--3007",
}

@inproceedings{zhang-etal-2020-improving,
    title = "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation",
    author = "Zhang, Biao  and
      Williams, Philip  and
      Titov, Ivan  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.148",
    doi = "10.18653/v1/2020.acl-main.148",
    pages = "1628--1639",
    abstract = "Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by {\textasciitilde}10 BLEU, approaching conventional pivot-based methods.",
}

@inproceedings{yang-etal-2021-multilingual,
    title = "Multilingual Agreement for Multilingual Neural Machine Translation",
    author = "Yang, Jian  and
      Yin, Yuwei  and
      Ma, Shuming  and
      Huang, Haoyang  and
      Zhang, Dongdong  and
      Li, Zhoujun  and
      Wei, Furu",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.31",
    doi = "10.18653/v1/2021.acl-short.31",
    pages = "233--239",
    abstract = "Although multilingual neural machine translation (MNMT) enables multiple language translations, the training process is based on independent multilingual objectives. Most multilingual models can not explicitly exploit different language pairs to assist each other, ignoring the relationships among them. In this work, we propose a novel agreement-based method to encourage multilingual agreement among different translation directions, which minimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines.",
}

@inproceedings{yang-etal-2021-improving-multilingual,
    title = "Improving Multilingual Translation by Representation and Gradient Regularization",
    author = "Yang, Yilin  and
      Eriguchi, Akiko  and
      Muzio, Alexandre  and
      Tadepalli, Prasad  and
      Lee, Stefan  and
      Hassan, Hany",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.578",
    doi = "10.18653/v1/2021.emnlp-main.578",
    pages = "7266--7279",
    abstract = "Multilingual Neural Machine Translation (NMT) enables one model to serve all translation directions, including ones that are unseen during training, i.e. zero-shot translation. Despite being theoretically attractive, current models often produce low quality translations {--} commonly failing to even produce outputs in the right target language. In this work, we observe that off-target translation is dominant even in strong multilingual systems, trained on massive multilingual corpora. To address this issue, we propose a joint approach to regularize NMT models at both representation-level and gradient-level. At the representation level, we leverage an auxiliary target language prediction task to regularize decoder outputs to retain information about the target language. At the gradient level, we leverage a small amount of direct data (in thousands of sentence pairs) to regularize model gradients. Our results demonstrate that our approach is highly effective in both reducing off-target translation occurrences and improving zero-shot translation performance by +5.59 and +10.38 BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our method also works well when the small amount of direct data is not available.",
}


@inproceedings{zhu-etal-2020-language,
    title = "Language-aware Interlingua for Multilingual Neural Machine Translation",
    author = "Zhu, Changfeng  and
      Yu, Heng  and
      Cheng, Shanbo  and
      Luo, Weihua",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.150",
    doi = "10.18653/v1/2020.acl-main.150",
    pages = "1650--1655",
    abstract = "Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages. However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained. In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture. The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair. Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.",
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{hendy2023good,
      title={How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation}, 
      author={Amr Hendy and Mohamed Abdelrehim and Amr Sharaf and Vikas Raunak and Mohamed Gabr and Hitokazu Matsushita and Young Jin Kim and Mohamed Afify and Hany Hassan Awadalla},
      year={2023},
      eprint={2302.09210},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Wu2023ExploringPE,
  title={Exploring Prompt Engineering with GPT Language Models for Document-Level Machine Translation: Insights and Findings},
  author={Yangjian Wu and Gang Hu},
  booktitle={Conference on Machine Translation},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265608105}
}

@inproceedings{Wu2023TreatingGM,
  title={Treating General MT Shared Task as a Multi-Domain Adaptation Problem: HW-TSC’s Submission to the WMT23 General MT Shared Task},
  author={Zhanglin Wu and Daimeng Wei and Zongyao Li and Zhengzhe Yu and Shaojun Li and Xiaoyu Chen and Hengchao Shang and Jiaxin Guo and Yuhao Xie and Lizhi Lei and Hao Yang and Yanfei Jiang},
  booktitle={Conference on Machine Translation},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265607964}
}
@inproceedings{Zhang2023IOLRM,
  title={IOL Research Machine Translation Systems for WMT23 General Machine Translation Shared Task},
  author={Wenbo Zhang},
  booktitle={Conference on Machine Translation},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265608114}
}
@inproceedings{Min2023YishuYA,
  title={Yishu: Yishu at WMT2023 Translation Task},
  author={Luo Min and Yixin Tan and Qiulin Chen},
  booktitle={Conference on Machine Translation},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265608033}
}
@inproceedings{Zeng2023AchievingSM,
  title={Achieving State-of-the-Art Multilingual Translation Model with Minimal Data and Parameters},
  author={Hui Zeng},
  booktitle={Conference on Machine Translation},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265607992}
}

@inproceedings{rikters2023aist,
  title={Aist airc submissions to the wmt23 shared task},
  author={Rikters, Mat{\=\i}ss and Miwa, Makoto},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={155--161},
  year={2023}
}
@inproceedings{popel2020cuni,
  title={CUNI English-Czech and English-Polish systems in WMT20: Robust document-level training},
  author={Popel, Martin},
  booktitle={Proceedings of the Fifth Conference on Machine Translation},
  pages={269--273},
  year={2020}
}
@inproceedings{jon2023cuni,
  title={CUNI at WMT23 General Translation Task: MT and a Genetic Algorithm},
  author={Jon, Josef and Popel, Martin and Bojar, Ond{\v{r}}ej},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={119--127},
  year={2023}
}
@inproceedings{zong2023gtcom,
  title={GTCOM and DLUT’s Neural Machine Translation Systems for WMT23},
  author={Zong, Hao},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={192--197},
  year={2023}
}
@inproceedings{li2023kyb,
  title={KYB General Machine Translation Systems for WMT23},
  author={Li, Ben and Matsuzaki, Yoko and Kalkar, Shivam},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={137--142},
  year={2023}
}
@inproceedings{rychly2023muni,
  title={MUNI-NLP Submission for Czech-Ukrainian Translation Task at WMT23},
  author={Rychl{\`y}, Pavel and Teslia, Yuliia},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={162--165},
  year={2023}
}
@inproceedings{deguchi2023naist,
  title={NAIST-NICT WMT’23 General MT Task Submission},
  author={Deguchi, Hiroyuki and Imamura, Kenji and Nishida, Yuto and Sakai, Yusuke and Vasselli, Justin and Watanabe, Taro},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={110--118},
  year={2023}
}
@inproceedings{freitag2023results,
  title={Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent},
  author={Freitag, Markus and Mathur, Nitika and Lo, Chi-kiu and Avramidis, Eleftherios and Rei, Ricardo and Thompson, Brian and Kocmi, Tom and Blain, Fr{\'e}d{\'e}ric and Deutsch, Daniel and Stewart, Craig and others},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={578--628},
  year={2023}
}
@inproceedings{molchanov2023promt,
  title={PROMT Systems for WMT23 Shared General Translation Task},
  author={Molchanov, Alexander and Kovalenko, Vladislav},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={150--154},
  year={2023}
}
@article{cruz2023samsung,
  title={Samsung R\&D Institute Philippines at WMT 2023},
  author={Cruz, Jan Christian Blaise},
  journal={arXiv preprint arXiv:2310.16322},
  year={2023}
}
@inproceedings{kudo2023skim,
  title={SKIM at WMT 2023 general translation task},
  author={Kudo, Keito and Ito, Takumi and Morishita, Makoto and Suzuki, Jun},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={128--136},
  year={2023}
}

@article{wu2023uva,
  title={UvA-MT's Participation in the WMT23 General Translation Shared Task},
  author={Wu, Di and Tan, Shaomu and Stap, David and Araabi, Ali and Monz, Christof},
  journal={arXiv preprint arXiv:2310.09946},
  year={2023}
}





@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@article{yuan2021wudaocorpora,
  title={Wudaocorpora: A super large-scale chinese corpora for pre-training language models},
  author={Yuan, Sha and Zhao, Hanyu and Du, Zhengxiao and Ding, Ming and Liu, Xiao and Cen, Yukuo and Zou, Xu and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  volume={2},
  pages={65--68},
  year={2021},
  publisher={Elsevier}
}


@inproceedings{yan2023transn,
  title={Transn’s Submission for CCMT 2023 Quality Estimation Task},
  author={Yan, Zeyu and Zhang, Wenbo and Deng, Qiaobo and Mao, Hongbao and Cai, Jie and He, Zhengyu},
  booktitle={China Conference on Machine Translation},
  pages={1--12},
  year={2023},
  organization={Springer}
}
@InProceedings{10.1007/978-981-99-7894-62,
author="Wu, Zhanglin
and Yu, Zhengzhe
and Li, Zongyao
and Wei, Daimeng
and Xie, Yuhao
and Chen, Xiaoyu
and Shang, Hengchao
and Guo, Jiaxin
and Rao, Zhiqiang
and Li, Shaojun
and peng, Song
and Lei, Lizhi
and Yang, Hao
and Jiang, Yanfei",
editor="Feng, Yang
and Feng, Chong",
title="HW-TSC's Neural Machine Translation System for CCMT 2023",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="13--27",
abstract="This paper presents Huawei Translation Service Center (HW-TSC)'s submission to the machine translation tasks of the 19th China Conference on Machine Translation (CCMT 2023). We participate in all machine translation tasks, including five bilingual machine translation tasks, the Belt and Road low-resource language machine translation task, Chinese-centric multilingual machine translation task and Chinese{\$}{\$}{\backslash}rightarrow {\$}{\$}{\textrightarrow}English zero-referencing machine translation task. Under different machine translation tasks, we adopt different methods to train the corresponding neural machine translation system. This paper mainly explains the model structure, data size and training method adopted by the translation systems, and gives the comparison of the evaluation results under different training methods.",
isbn="978-981-99-7894-6"
}

@InProceedings{10.1007/978-981-99-7894-64,
author="Liu, Fan
and Zhao, Yahui
and Jin, Guozhe
and Lu, Xinghua
and Jin, Zhejun
and Cui, Rongyi",
editor="Feng, Yang
and Feng, Chong",
title="Korean-Chinese Machine Translation Method Based on Independent Language Features",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="37--49",
abstract="Currently, the mainstream approach for Korean-Chinese machine translation is to improve the performance of translation models by building upon existing deep learning models. However, there has been insufficient focus on leveraging the linguistic relationship between Korean and Chinese to enhance translation quality. This paper proposes an improvement method that uses the natural connection between the two languages-the Sino-Korean words. Firstly, we shorten the word vector distance of the Sino-Korean words in the semantic space to extract the independent language features of the Sino-Korean words. Secondly, we propose a fine-tuning model to solve mistranslation and translation ambiguity issues caused by 1:N Sino-Korean word pairs. Lastly, we add the coverage loss function to the machine translation model to reduce duplication of translation. This paper conducted experiments on the datasets SWRC and East Asia Daily. Compared with the baseline model, the score of Chinese-to-Korean translation on the SWRC and the East Asia Daily increased by 6.41 BLEU points and 4.09 BLEU points respectively; and the score of Korean-to-Chinese translation increased by 8.43 BLEU points and 4.13 BLEU points respectively.",
isbn="978-981-99-7894-6"
}
%&&&&&&&&&&&&&&&&&&&
@InProceedings{10.1007/978-981-99-7894-65,
author="Lai, Zhejian
and Geng, Xiang
and Zhang, Yu
and Chen, Jiajun
and Huang, Shujian",
editor="Feng, Yang
and Feng, Chong",
title="NJUNLP's Submission for CCMT 2023 Quality Estimation Task",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="50--56",
abstract="Quality Estimation is a task aiming to estimate the quality of translations without relying on any references. This paper describes our submission for CCMT 2023 quality estimation sentence-level task for English-to-Chinese (EN-ZH). Due to the challenges of costly annotations and small dataset sizes in the QE field, many researchers have attempted to leverage rich parallel corpora for unsupervised learning through methods such as uncertainty quantification, and data augmentation. Existing mainstream unsupervised QE methods exhibit good diversity and variability, so we test these methods individually as well as their ensemble effect.",
isbn="978-981-99-7894-6"
}
@inproceedings{cui2021directqe,
  title={Directqe: Direct pretraining for machine translation quality estimation},
  author={Cui, Qu and Huang, Shujian and Li, Jiahuan and Geng, Xiang and Zheng, Zaixiang and Huang, Guoping and Chen, Jiajun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12719--12727},
  year={2021}
}
@inproceedings{snover2006study,
  title={A study of translation edit rate with targeted human annotation},
  author={Snover, Matthew and Dorr, Bonnie and Schwartz, Richard and Micciulla, Linnea and Makhoul, John},
  booktitle={Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers},
  pages={223--231},
  year={2006}
}
@inproceedings{zheng2021self,
  title={Self-supervised quality estimation for machine translation},
  author={Zheng, Yuanhang and Tan, Zhixing and Zhang, Meng and Maimaiti, Mieradilijiang and Luan, Huanbo and Sun, Maosong and Liu, Qun and Liu, Yang},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3322--3334},
  year={2021}
}
@InProceedings{10.1007/978-981-99-7894-66,
author="Zhang, Rui
and Yuan, Jinghao
and Huang, Hui
and Yang, Muyun
and Zhao, Tiejun",
editor="Feng, Yang
and Feng, Chong",
title="HIT-MI {\&}T Lab's Submission to CCMT 2023 Automatic Post-editing Task",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="57--68",
abstract="Automatic post-editing (APE) aims to automatically correct the outputs of the machine translation system by learning from manual correction examples. We present the system developed by HIT-MI {\&}T Lab for the CCMT 2023 APE task of Chinese-English direction. We use mBART as the backbone model, and explore different techniques to create synthetic data, including domain selection, forward translation and data augmentation via large language model. Multi-model ensemble is also adopted in our final system. The experiment results on the development set demonstrate the effectiveness of our proposed method.",
isbn="978-981-99-7894-6"
}
@InProceedings{10.1007/978-981-99-7894-67,
author="Ye, Na
and Li, Jiaxin",
editor="Feng, Yang
and Feng, Chong",
title="A k-Nearest Neighbor Approach for Domain-Specific Translation Quality Estimation",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="69--80",
abstract="Translation quality estimation (QE) is used to assess the quality of machine translation (MT) output without using reference translations. Although QE technology in general domain has made significant progress, it still faces challenges in specific domains due to limited data availability and expensive annotation costs. To address this issue, this paper proposes a kNN-QE method, which provides instance-based augmentation for the model by querying the built datastore during prediction. This method does not require an explicit training process and improves the prediction accuracy of the model without spending additional time and computing resources to train the model. This paper further improves the model performance by adjusting the loss function to alleviate the problem of QE data label bias. Experiments on two domain-specific datasets show that the proposed method achieves significant improvements over the baseline method on word-level QE tasks.",
isbn="978-981-99-7894-6"
}
@InProceedings{10.1007/978-981-99-7894-68,
author="Ye, Na
and Fu, Gen",
editor="Feng, Yang
and Feng, Chong",
title="WSA: A Unified Framework for Word and Sentence Autocompletion in Interactive Machine Translation",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="81--93",
abstract="Interactive machine translation (IMT) is a process that leverages collaboration between human translators and machine translation models to produce high-quality translations and improve translation efficiency. Automatic completion is a critical function of IMT, which generates alternative translation for inappropriate words or segments based on human feedback for the translation. There are two limitations in previous research on autocompletion. Firstly, they treat these two types of autocompletion tasks as independent tasks, thus ignoring the potential correlation between them. The second limitation is that they only focus on completing translations based on human feedback, ignoring the role of the initial translation. In this paper, we propose a novel word and sentence autocompletion (WSA) method, which jointly models word and sentence autocompletion tasks. By means of joint modeling, the proposed method not only enables the generation of translations at both word and sentence level, but also increases the accuracy of the translations. In addition, we further improve the accuracy of translation autocompletion by utilizing the initial translation to enhance the semantic representation of the source sentence. Experimental results show that our method significantly outperforms the baselines by 10.34{\%} in accuracy for word-level task and 1.86 BLEU score for sentence-level task.",
isbn="978-981-99-7894-6"
}

@InProceedings{10.1007/978-981-99-7894-610,
author="Zhang, Zhiyang
and Zhang, Yaping
and Xiang, Lu
and Zhao, Yang
and Zhou, Yu
and Zong, Chengqing",
editor="Feng, Yang
and Feng, Chong",
title="A Novel Dataset and Benchmark Analysis on Document Image Translation",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="103--115",
abstract="Document image translation (DIT) deserves more attention on account of its importance in many real-world scenarios. It is a challenging task because of the layout degeneration and noisy text translation problems caused by the optical character recognition (OCR) model. Moreover, due to the task-specific annotation, existing document image datasets usually do not support in-depth DIT analysis and model development. So, to motivate a broader investigation, this paper presents a dataset named DITrans, which provides fine-grained annotations for English-to-Chinese DIT task. It contains 2.8k English document images in three domains: political report, scientific article and paper book. Each document image has been annotated with layout structure, source text and translation references. Based on DITrans, a novel framework, which strengthens the conventional OCR-Translation cascade in layout awareness and noise robustness for better DIT, has been proposed. Furthermore, benchmark evaluations and detailed analysis based on this framework have been conducted. The evaluations and analysis results demonstrate that the dataset is very practical and can facilitate full-stack analysis and long-term research on DIT.",
isbn="978-981-99-7894-6"
}
@InProceedings{10.1007/978-981-99-7894-611,
author="Guo, Bokai
and Feng, Chong
and Liu, Fang
and Li, Xinyan
and Wang, Xiaomei",
editor="Feng, Yang
and Feng, Chong",
title="Joint Contrastive Learning for Factual Consistency Evaluation of Cross-Lingual Abstract Summarization",
booktitle="Machine Translation",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="116--127",
abstract="Current summarization models tend to generate erroneous or irrelevant summaries, i.e., factual inconsistency, which undoubtedly hinders the real-world application of summarization models. The difficulty in language alignment makes factual inconsistency in cross-lingual summarization (CLS) more common and factual consistency checking more challenging. Research on factual consistency has paid little attention to CLS due to the above difficulties, focusing mainly on monolingual summarization (MS). In this paper, we investigate the cross-lingual domain and propose a weakly supervised factual consistency evaluation model for CLS. In particular, we automatically synthesize large-scale datasets by a series of rule-based text transformations and manually annotate the test and validation sets. In addition, we also train the model jointly with contrastive learning to enhance the model's ability to recognize factual errors. The experimental results on the manually annotated test set show that our model can effectively identify the consistency between the summaries and the source documents and outperform the baseline models.",
isbn="978-981-99-7894-6"
}


